{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.utils import excute_cmd, execute_cmd_realtime\n",
    "\n",
    "# To allow auto reload to this notebook after modifying any external file imported\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--train_path TRAIN_PATH] [--valid_path VALID_PATH]\n",
      "                [--output OUTPUT] [--experiment_name EXPERIMENT_NAME]\n",
      "                [--network_name NETWORK_NAME] [--max_epochs MAX_EPOCHS]\n",
      "                [--batch_size BATCH_SIZE] [--base_lr BASE_LR]\n",
      "                [--patience PATIENCE] [--img_size IMG_SIZE] [--seed SEED]\n",
      "                [--verbose VERBOSE] [--normalize_attn]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --train_path TRAIN_PATH\n",
      "                        root dir for training data\n",
      "  --valid_path VALID_PATH\n",
      "                        root dir for validation data\n",
      "  --output OUTPUT       output dir for saving results\n",
      "  --experiment_name EXPERIMENT_NAME\n",
      "                        experiment name\n",
      "  --network_name NETWORK_NAME\n",
      "                        network name\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        maximum epoch number to train\n",
      "  --batch_size BATCH_SIZE\n",
      "                        batch_size per gpu\n",
      "  --base_lr BASE_LR     network learning rate\n",
      "  --patience PATIENCE   patience for lr and early stopping scheduler\n",
      "  --img_size IMG_SIZE   input image size of network input\n",
      "  --seed SEED           random seed value\n",
      "  --verbose VERBOSE     verbose value [0:2]\n",
      "  --normalize_attn      if True, attention map is normalized by softmax;\n",
      "                        otherwise use sigmoid. This is only for certain\n",
      "                        networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --help'\n",
    "\n",
    "# run the function to excute the command\n",
    "excute_cmd(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be excuting all training commands here to keep track of all experiments. This notebook will only contain training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 14:28:06.793 | INFO     | __main__:<module>:36 - Excuting training pipeline with 20 epochs...\n",
      "2023-11-27 14:28:06.804 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/train\n",
      "2023-11-27 14:28:06.869 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/val\n",
      "2023-11-27 14:28:06.917 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for train split.\n",
      "2023-11-27 14:28:06.917 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for val split.\n",
      "2023-11-27 14:28:06.953 | INFO     | experiments.ClassifierExperiment:__init__:57 - Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "2023-11-27 14:28:15.096 | INFO     | networks.VGG16:__init__:107 - Using VGG16_BN_Attention with configurations: num_classes='2'\n",
      "2023-11-27 14:28:17.769 | INFO     | experiments.ClassifierExperiment:run:233 - Experiment started.\n",
      "2023-11-27 14:48:27.878 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 01/20 | epoch time: 20.0m 10.11s | lr: 1.00000e-04 | train/loss: 0.43498 | val/loss: 0.39895 | val/accuracy: 0.81955 | val/AUC: 0.82061 | val/Kappa: 0.63979\n",
      "2023-11-27 15:06:20.925 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 02/20 | epoch time: 17.0m 53.04s | lr: 1.00000e-04 | train/loss: 0.38360 | val/loss: 0.35237 | val/accuracy: 0.84273 | val/AUC: 0.84182 | val/Kappa: 0.68482\n",
      "2023-11-27 15:26:32.411 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 03/20 | epoch time: 20.0m 11.48s | lr: 1.00000e-04 | train/loss: 0.35776 | val/loss: 0.37009 | val/accuracy: 0.83456 | val/AUC: 0.83273 | val/Kappa: 0.66784\n",
      "2023-11-27 15:41:04.665 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 04/20 | epoch time: 14.0m 32.25s | lr: 1.00000e-04 | train/loss: 0.34469 | val/loss: 0.32532 | val/accuracy: 0.85880 | val/AUC: 0.85752 | val/Kappa: 0.71681\n",
      "2023-11-27 15:55:02.513 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 05/20 | epoch time: 13.0m 57.84s | lr: 1.00000e-04 | train/loss: 0.32817 | val/loss: 0.33374 | val/accuracy: 0.85564 | val/AUC: 0.85580 | val/Kappa: 0.71130\n",
      "2023-11-27 16:09:10.636 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 06/20 | epoch time: 14.0m 8.12s | lr: 1.00000e-04 | train/loss: 0.31857 | val/loss: 0.32429 | val/accuracy: 0.85248 | val/AUC: 0.85117 | val/Kappa: 0.70412\n",
      "2023-11-27 16:23:41.658 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 07/20 | epoch time: 14.0m 31.02s | lr: 1.00000e-04 | train/loss: 0.30739 | val/loss: 0.32375 | val/accuracy: 0.85933 | val/AUC: 0.85833 | val/Kappa: 0.71803\n",
      "2023-11-27 16:37:42.469 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 08/20 | epoch time: 14.0m 0.8083s | lr: 1.00000e-04 | train/loss: 0.30323 | val/loss: 0.30581 | val/accuracy: 0.86117 | val/AUC: 0.86064 | val/Kappa: 0.72198\n",
      "2023-11-27 16:52:02.638 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 09/20 | epoch time: 14.0m 20.17s | lr: 1.00000e-04 | train/loss: 0.28792 | val/loss: 0.33054 | val/accuracy: 0.84932 | val/AUC: 0.84850 | val/Kappa: 0.69807\n",
      "2023-11-27 17:06:34.231 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 10/20 | epoch time: 14.0m 31.59s | lr: 1.00000e-04 | train/loss: 0.28582 | val/loss: 0.31132 | val/accuracy: 0.87276 | val/AUC: 0.87286 | val/Kappa: 0.74552\n",
      "2023-11-27 17:21:04.340 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 11/20 | epoch time: 14.0m 30.11s | lr: 1.00000e-04 | train/loss: 0.27303 | val/loss: 0.30638 | val/accuracy: 0.86723 | val/AUC: 0.86741 | val/Kappa: 0.73449\n",
      "2023-11-27 17:36:05.242 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 12/20 | epoch time: 15.0m 0.898s | lr: 1.00000e-04 | train/loss: 0.26494 | val/loss: 0.30514 | val/accuracy: 0.87434 | val/AUC: 0.87421 | val/Kappa: 0.74856\n",
      "2023-11-27 17:51:28.357 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 13/20 | epoch time: 15.0m 23.11s | lr: 1.00000e-04 | train/loss: 0.26549 | val/loss: 0.33668 | val/accuracy: 0.85827 | val/AUC: 0.85898 | val/Kappa: 0.71688\n",
      "2023-11-27 18:07:04.473 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 14/20 | epoch time: 15.0m 36.11s | lr: 1.00000e-04 | train/loss: 0.25651 | val/loss: 0.29713 | val/accuracy: 0.87987 | val/AUC: 0.87931 | val/Kappa: 0.75942\n",
      "2023-11-27 18:21:45.311 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 15/20 | epoch time: 14.0m 40.83s | lr: 1.00000e-04 | train/loss: 0.24935 | val/loss: 0.30871 | val/accuracy: 0.87092 | val/AUC: 0.87077 | val/Kappa: 0.74170\n",
      "2023-11-27 18:36:04.153 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 16/20 | epoch time: 14.0m 18.84s | lr: 1.00000e-04 | train/loss: 0.24268 | val/loss: 0.29105 | val/accuracy: 0.88066 | val/AUC: 0.88026 | val/Kappa: 0.76108\n",
      "2023-11-27 18:50:21.588 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 17/20 | epoch time: 14.0m 17.43s | lr: 1.00000e-04 | train/loss: 0.23453 | val/loss: 0.29464 | val/accuracy: 0.87381 | val/AUC: 0.87331 | val/Kappa: 0.74732\n",
      "2023-11-27 19:04:45.114 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 18/20 | epoch time: 14.0m 23.52s | lr: 1.00000e-04 | train/loss: 0.23452 | val/loss: 0.30964 | val/accuracy: 0.86565 | val/AUC: 0.86525 | val/Kappa: 0.73102\n",
      "2023-11-27 19:19:50.157 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 19/20 | epoch time: 15.0m 5.04s | lr: 1.00000e-04 | train/loss: 0.23275 | val/loss: 0.29927 | val/accuracy: 0.87592 | val/AUC: 0.87605 | val/Kappa: 0.75185\n",
      "2023-11-27 19:34:17.819 | INFO     | experiments.ClassifierExperiment:run:248 - Epoch: 20/20 | epoch time: 14.0m 27.66s | lr: 1.00000e-04 | train/loss: 0.21435 | val/loss: 0.30712 | val/accuracy: 0.86855 | val/AUC: 0.86765 | val/Kappa: 0.73656\n",
      "2023-11-27 19:34:17.822 | INFO     | callbacks.early_stopping:save_checkpoint:34 - Saving model parameters...\n",
      "2023-11-27 19:34:20.678 | INFO     | experiments.ClassifierExperiment:run:269 - Run complete. Total time: 05:06:02\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 2 out of 5\n",
      "EarlyStopping counter: 3 out of 5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "EarlyStopping counter: 2 out of 5\n",
      "EarlyStopping counter: 3 out of 5\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierExperiment\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"20\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"1\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting the same network with same configuration but using `--normalize_attn` flag to normalize the network by using softmax instead of sigmoid for attention map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 20:34:37.132 | INFO     | __main__:<module>:38 - Excuting training pipeline with 20 epochs...\n",
      "2023-11-27 20:34:37.143 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/train\n",
      "2023-11-27 20:34:37.206 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/val\n",
      "2023-11-27 20:34:37.224 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for train split.\n",
      "2023-11-27 20:34:37.225 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for val split.\n",
      "2023-11-27 20:34:37.260 | INFO     | experiments.ClassifierExperiment:__init__:57 - Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "2023-11-27 20:34:39.774 | INFO     | experiments.ClassifierExperiment:__init__:73 - Normalizing the attention network.\n",
      "2023-11-27 20:34:39.775 | INFO     | networks.VGG16:__init__:107 - Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='True'\n",
      "2023-11-27 20:34:42.370 | INFO     | experiments.ClassifierExperiment:run:238 - Experiment started.\n",
      "2023-11-27 20:49:33.794 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 01/20 | epoch time: 14.0m 51.42s | lr: 1.00000e-04 | train/loss: 0.43469 | val/loss: 0.42608 | val/accuracy: 0.79557 | val/AUC: 0.79724 | val/Kappa: 0.59243\n",
      "2023-11-27 21:03:43.200 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 02/20 | epoch time: 14.0m 9.399s | lr: 1.00000e-04 | train/loss: 0.38312 | val/loss: 0.34410 | val/accuracy: 0.84905 | val/AUC: 0.84835 | val/Kappa: 0.69762\n",
      "2023-11-27 21:17:36.721 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 03/20 | epoch time: 13.0m 53.52s | lr: 1.00000e-04 | train/loss: 0.35682 | val/loss: 0.39966 | val/accuracy: 0.81823 | val/AUC: 0.81576 | val/Kappa: 0.63459\n",
      "2023-11-27 21:17:36.724 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-27 21:32:35.678 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 04/20 | epoch time: 14.0m 58.95s | lr: 1.00000e-04 | train/loss: 0.34426 | val/loss: 0.33176 | val/accuracy: 0.85063 | val/AUC: 0.84924 | val/Kappa: 0.70037\n",
      "2023-11-27 21:47:12.039 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 05/20 | epoch time: 14.0m 36.36s | lr: 1.00000e-04 | train/loss: 0.32736 | val/loss: 0.32459 | val/accuracy: 0.85959 | val/AUC: 0.85970 | val/Kappa: 0.71918\n",
      "2023-11-27 22:01:55.855 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 06/20 | epoch time: 14.0m 43.81s | lr: 1.00000e-04 | train/loss: 0.31933 | val/loss: 0.32422 | val/accuracy: 0.85406 | val/AUC: 0.85285 | val/Kappa: 0.70734\n",
      "2023-11-27 22:16:10.583 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 07/20 | epoch time: 14.0m 14.73s | lr: 1.00000e-04 | train/loss: 0.30801 | val/loss: 0.31618 | val/accuracy: 0.85880 | val/AUC: 0.85848 | val/Kappa: 0.71736\n",
      "2023-11-27 22:30:28.351 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 08/20 | epoch time: 14.0m 17.76s | lr: 1.00000e-04 | train/loss: 0.30153 | val/loss: 0.30725 | val/accuracy: 0.86670 | val/AUC: 0.86625 | val/Kappa: 0.73310\n",
      "2023-11-27 22:46:25.441 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 09/20 | epoch time: 15.0m 57.09s | lr: 1.00000e-04 | train/loss: 0.28688 | val/loss: 0.31063 | val/accuracy: 0.86407 | val/AUC: 0.86360 | val/Kappa: 0.72782\n",
      "2023-11-27 22:46:25.445 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-27 23:01:30.578 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 10/20 | epoch time: 15.0m 5.133s | lr: 1.00000e-04 | train/loss: 0.28275 | val/loss: 0.28928 | val/accuracy: 0.87671 | val/AUC: 0.87646 | val/Kappa: 0.75324\n",
      "2023-11-27 23:15:54.411 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 11/20 | epoch time: 14.0m 23.83s | lr: 1.00000e-04 | train/loss: 0.27535 | val/loss: 0.31320 | val/accuracy: 0.85906 | val/AUC: 0.85955 | val/Kappa: 0.71834\n",
      "2023-11-27 23:15:54.414 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-27 23:30:38.228 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 12/20 | epoch time: 14.0m 43.81s | lr: 1.00000e-04 | train/loss: 0.26284 | val/loss: 0.31365 | val/accuracy: 0.87039 | val/AUC: 0.86981 | val/Kappa: 0.74042\n",
      "2023-11-27 23:30:38.231 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-27 23:50:45.821 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 13/20 | epoch time: 20.0m 7.589s | lr: 1.00000e-04 | train/loss: 0.26392 | val/loss: 0.34000 | val/accuracy: 0.85643 | val/AUC: 0.85722 | val/Kappa: 0.71325\n",
      "2023-11-27 23:50:45.824 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 3 out of 5\n",
      "2023-11-28 00:06:21.309 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 14/20 | epoch time: 15.0m 35.48s | lr: 1.00000e-04 | train/loss: 0.25401 | val/loss: 0.31068 | val/accuracy: 0.87355 | val/AUC: 0.87327 | val/Kappa: 0.74691\n",
      "2023-11-28 00:06:21.313 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 4 out of 5\n",
      "2023-11-28 00:22:07.879 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 15/20 | epoch time: 15.0m 46.57s | lr: 1.00000e-04 | train/loss: 0.24622 | val/loss: 0.30214 | val/accuracy: 0.87355 | val/AUC: 0.87332 | val/Kappa: 0.74693\n",
      "2023-11-28 00:22:07.883 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 5 out of 5\n",
      "2023-11-28 00:22:07.884 | INFO     | callbacks.early_stopping:save_checkpoint:38 - Saving model parameters...\n",
      "2023-11-28 00:22:09.930 | INFO     | experiments.ClassifierExperiment:run:274 - Run complete. Total time: 03:47:27\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierExperiment\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --normalize_attn \\\n",
    "                        --max_epochs \"20\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"1\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced model weights initialization as the official implementation using kaiming_normal, and re-run the first experiment on more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28 17:07:41.238 | INFO     | __main__:<module>:38 - Excuting training pipeline with 30 epochs...\n",
      "2023-11-28 17:07:41.263 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/train\n",
      "2023-11-28 17:07:41.365 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/val\n",
      "2023-11-28 17:07:41.388 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for train split.\n",
      "2023-11-28 17:07:41.388 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for val split.\n",
      "2023-11-28 17:07:41.780 | INFO     | experiments.ClassifierExperiment:__init__:57 - Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "2023-11-28 17:07:51.702 | INFO     | networks.VGG16:__init__:107 - Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "2023-11-28 17:07:59.650 | INFO     | experiments.ClassifierExperiment:run:238 - Experiment started.\n",
      "2023-11-28 17:24:28.902 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 01/30 | epoch time: 16.0m 29.25s | lr: 1.00000e-04 | train/loss: 0.44464 | val/loss: 0.38168 | val/accuracy: 0.83430 | val/AUC: 0.83456 | val/Kappa: 0.66870\n",
      "2023-11-28 17:38:33.944 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 02/30 | epoch time: 14.0m 5.022s | lr: 1.00000e-04 | train/loss: 0.38309 | val/loss: 0.35980 | val/accuracy: 0.84062 | val/AUC: 0.83963 | val/Kappa: 0.68055\n",
      "2023-11-28 17:52:34.298 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 03/30 | epoch time: 14.0m 0.3512s | lr: 1.00000e-04 | train/loss: 0.36126 | val/loss: 0.34261 | val/accuracy: 0.84352 | val/AUC: 0.84377 | val/Kappa: 0.68713\n",
      "2023-11-28 18:07:08.865 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 04/30 | epoch time: 14.0m 34.56s | lr: 1.00000e-04 | train/loss: 0.34272 | val/loss: 0.34805 | val/accuracy: 0.85300 | val/AUC: 0.85337 | val/Kappa: 0.70616\n",
      "2023-11-28 18:07:08.868 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-28 18:22:03.732 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 05/30 | epoch time: 14.0m 54.86s | lr: 1.00000e-04 | train/loss: 0.33512 | val/loss: 0.33835 | val/accuracy: 0.85432 | val/AUC: 0.85480 | val/Kappa: 0.70886\n",
      "2023-11-28 18:36:24.869 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 06/30 | epoch time: 14.0m 21.13s | lr: 1.00000e-04 | train/loss: 0.31957 | val/loss: 0.33796 | val/accuracy: 0.85142 | val/AUC: 0.85155 | val/Kappa: 0.70286\n",
      "2023-11-28 18:50:53.634 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 07/30 | epoch time: 14.0m 28.76s | lr: 1.00000e-04 | train/loss: 0.30861 | val/loss: 0.32840 | val/accuracy: 0.85801 | val/AUC: 0.85731 | val/Kappa: 0.71556\n",
      "2023-11-28 19:05:53.746 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 08/30 | epoch time: 15.0m 0.1083s | lr: 1.00000e-04 | train/loss: 0.30412 | val/loss: 0.30080 | val/accuracy: 0.87460 | val/AUC: 0.87436 | val/Kappa: 0.74903\n",
      "2023-11-28 19:21:48.274 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 09/30 | epoch time: 15.0m 54.53s | lr: 1.00000e-04 | train/loss: 0.28958 | val/loss: 0.33215 | val/accuracy: 0.85353 | val/AUC: 0.85221 | val/Kappa: 0.70622\n",
      "2023-11-28 19:21:48.276 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-28 19:36:38.687 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 10/30 | epoch time: 14.0m 50.41s | lr: 1.00000e-04 | train/loss: 0.28695 | val/loss: 0.33678 | val/accuracy: 0.84747 | val/AUC: 0.84652 | val/Kappa: 0.69430\n",
      "2023-11-28 19:36:38.689 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-28 19:51:14.349 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 11/30 | epoch time: 14.0m 35.66s | lr: 1.00000e-04 | train/loss: 0.27773 | val/loss: 0.29645 | val/accuracy: 0.87540 | val/AUC: 0.87501 | val/Kappa: 0.75054\n",
      "2023-11-28 20:05:49.366 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 12/30 | epoch time: 14.0m 35.01s | lr: 1.00000e-04 | train/loss: 0.26992 | val/loss: 0.30154 | val/accuracy: 0.87645 | val/AUC: 0.87625 | val/Kappa: 0.75274\n",
      "2023-11-28 20:05:49.368 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-28 20:22:24.664 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 13/30 | epoch time: 16.0m 35.29s | lr: 1.00000e-04 | train/loss: 0.26184 | val/loss: 0.28960 | val/accuracy: 0.87882 | val/AUC: 0.87841 | val/Kappa: 0.75738\n",
      "2023-11-28 20:36:26.600 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 14/30 | epoch time: 14.0m 1.932s | lr: 1.00000e-04 | train/loss: 0.25735 | val/loss: 0.30609 | val/accuracy: 0.87171 | val/AUC: 0.87201 | val/Kappa: 0.74351\n",
      "2023-11-28 20:36:26.602 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-28 20:50:18.895 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 15/30 | epoch time: 13.0m 52.29s | lr: 1.00000e-04 | train/loss: 0.24644 | val/loss: 0.30521 | val/accuracy: 0.87381 | val/AUC: 0.87372 | val/Kappa: 0.74752\n",
      "2023-11-28 20:50:18.897 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-28 21:04:18.066 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 16/30 | epoch time: 13.0m 59.17s | lr: 1.00000e-04 | train/loss: 0.24304 | val/loss: 0.29506 | val/accuracy: 0.87856 | val/AUC: 0.87809 | val/Kappa: 0.75683\n",
      "2023-11-28 21:04:18.069 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 3 out of 5\n",
      "2023-11-28 21:18:21.322 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 17/30 | epoch time: 14.0m 3.251s | lr: 1.00000e-04 | train/loss: 0.23787 | val/loss: 0.33919 | val/accuracy: 0.85880 | val/AUC: 0.85984 | val/Kappa: 0.71812\n",
      "2023-11-28 21:18:21.324 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 4 out of 5\n",
      "2023-11-28 21:32:54.343 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 18/30 | epoch time: 14.0m 33.02s | lr: 1.00000e-04 | train/loss: 0.23127 | val/loss: 0.28921 | val/accuracy: 0.87724 | val/AUC: 0.87631 | val/Kappa: 0.75397\n",
      "2023-11-28 21:47:00.414 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 19/30 | epoch time: 14.0m 6.068s | lr: 1.00000e-04 | train/loss: 0.22717 | val/loss: 0.29211 | val/accuracy: 0.88066 | val/AUC: 0.88066 | val/Kappa: 0.76127\n",
      "2023-11-28 21:47:00.417 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-28 22:00:48.042 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 20/30 | epoch time: 13.0m 47.62s | lr: 1.00000e-04 | train/loss: 0.21986 | val/loss: 0.29665 | val/accuracy: 0.87987 | val/AUC: 0.87932 | val/Kappa: 0.75943\n",
      "2023-11-28 22:00:48.044 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-28 22:15:13.069 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 21/30 | epoch time: 14.0m 25.02s | lr: 1.00000e-04 | train/loss: 0.21286 | val/loss: 0.29721 | val/accuracy: 0.87935 | val/AUC: 0.87904 | val/Kappa: 0.75849\n",
      "2023-11-28 22:15:13.072 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 3 out of 5\n",
      "2023-11-28 22:29:53.649 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 22/30 | epoch time: 14.0m 40.58s | lr: 1.00000e-04 | train/loss: 0.21292 | val/loss: 0.31480 | val/accuracy: 0.87777 | val/AUC: 0.87803 | val/Kappa: 0.75561\n",
      "2023-11-28 22:29:53.651 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 4 out of 5\n",
      "2023-11-28 22:44:16.075 | INFO     | experiments.ClassifierExperiment:run:253 - Epoch: 23/30 | epoch time: 14.0m 22.42s | lr: 1.00000e-04 | train/loss: 0.20842 | val/loss: 0.30428 | val/accuracy: 0.87487 | val/AUC: 0.87362 | val/Kappa: 0.74905\n",
      "2023-11-28 22:44:16.078 | INFO     | callbacks.early_stopping:early_stop:27 - EarlyStopping counter: 5 out of 5\n",
      "2023-11-28 22:44:16.082 | INFO     | callbacks.early_stopping:save_checkpoint:38 - Saving model parameters...\n",
      "2023-11-28 22:44:20.097 | INFO     | experiments.ClassifierExperiment:run:274 - Run complete. Total time: 05:36:20\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierExperiment\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"30\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"1\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting `AdvancedHairAugmentation` in the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 17:44:20.598 | INFO     | __main__:<module>:38 - Excuting training pipeline with 30 epochs...\n",
      "2023-11-30 17:44:20.629 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/train\n",
      "2023-11-30 17:44:20.706 | INFO     | data_prep.dataset_loader:LoadData:24 - Loading the data from ../datasets/challenge1/val\n",
      "2023-11-30 17:44:20.727 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for train split.\n",
      "2023-11-30 17:44:20.727 | INFO     | data_prep.dataset:__init__:35 - Creating a Dataset instance for val split.\n",
      "2023-11-30 17:44:20.791 | INFO     | experiments.ClassifierExperiment:__init__:65 - Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "2023-11-30 17:44:31.335 | INFO     | networks.VGG16:__init__:107 - Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "2023-11-30 17:44:34.680 | INFO     | experiments.ClassifierExperiment:run:352 - Experiment started.\n",
      "2023-11-30 18:02:54.177 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from inf to 0.371382. Saving model ...\n",
      "2023-11-30 18:02:54.693 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 01/30 | epoch time: 18.0m 19.49s | lr: 1.00000e-04 | train/loss: 0.44285 | val/loss: 0.37138 | val/accuracy: 0.83693 | val/AUC: 0.83703 | val/Kappa: 0.67386\n",
      "2023-11-30 18:18:37.608 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.371382 to 0.356639. Saving model ...\n",
      "2023-11-30 18:18:37.870 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 02/30 | epoch time: 15.0m 42.91s | lr: 1.00000e-04 | train/loss: 0.38084 | val/loss: 0.35664 | val/accuracy: 0.84826 | val/AUC: 0.84744 | val/Kappa: 0.69596\n",
      "2023-11-30 18:35:22.804 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.356639 to 0.333056. Saving model ...\n",
      "2023-11-30 18:35:23.543 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 03/30 | epoch time: 16.0m 44.93s | lr: 1.00000e-04 | train/loss: 0.35913 | val/loss: 0.33306 | val/accuracy: 0.85564 | val/AUC: 0.85576 | val/Kappa: 0.71128\n",
      "2023-11-30 18:55:39.189 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-30 18:55:39.190 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 04/30 | epoch time: 20.0m 15.64s | lr: 1.00000e-04 | train/loss: 0.34069 | val/loss: 0.35323 | val/accuracy: 0.84563 | val/AUC: 0.84641 | val/Kappa: 0.69167\n",
      "2023-11-30 19:17:34.888 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.333056 to 0.314991. Saving model ...\n",
      "2023-11-30 19:17:35.206 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 05/30 | epoch time: 21.0m 55.69s | lr: 1.00000e-04 | train/loss: 0.33553 | val/loss: 0.31499 | val/accuracy: 0.86091 | val/AUC: 0.86012 | val/Kappa: 0.72131\n",
      "2023-11-30 19:35:19.597 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-30 19:35:19.617 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 06/30 | epoch time: 17.0m 44.39s | lr: 1.00000e-04 | train/loss: 0.32107 | val/loss: 0.32611 | val/accuracy: 0.85116 | val/AUC: 0.85131 | val/Kappa: 0.70235\n",
      "2023-11-30 20:09:05.031 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-30 20:09:05.036 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 07/30 | epoch time: 33.0m 45.41s | lr: 1.00000e-04 | train/loss: 0.30881 | val/loss: 0.32233 | val/accuracy: 0.86301 | val/AUC: 0.86199 | val/Kappa: 0.72541\n",
      "2023-11-30 20:25:37.764 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.314991 to 0.302132. Saving model ...\n",
      "2023-11-30 20:25:38.094 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 08/30 | epoch time: 16.0m 32.72s | lr: 1.00000e-04 | train/loss: 0.30284 | val/loss: 0.30213 | val/accuracy: 0.87171 | val/AUC: 0.87126 | val/Kappa: 0.74313\n",
      "2023-11-30 20:44:22.827 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-30 20:44:22.837 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 09/30 | epoch time: 18.0m 44.72s | lr: 1.00000e-04 | train/loss: 0.29841 | val/loss: 0.32310 | val/accuracy: 0.85643 | val/AUC: 0.85528 | val/Kappa: 0.71213\n",
      "2023-11-30 21:01:00.166 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-30 21:01:00.168 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 10/30 | epoch time: 16.0m 37.33s | lr: 1.00000e-04 | train/loss: 0.28994 | val/loss: 0.33192 | val/accuracy: 0.85511 | val/AUC: 0.85404 | val/Kappa: 0.70954\n",
      "2023-11-30 21:21:06.595 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 3 out of 5\n",
      "2023-11-30 21:21:06.602 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 11/30 | epoch time: 20.0m 6.423s | lr: 1.00000e-04 | train/loss: 0.27868 | val/loss: 0.39811 | val/accuracy: 0.83641 | val/AUC: 0.83414 | val/Kappa: 0.67126\n",
      "2023-11-30 21:41:15.695 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 4 out of 5\n",
      "2023-11-30 21:41:15.701 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 12/30 | epoch time: 20.0m 9.089s | lr: 1.00000e-04 | train/loss: 0.27086 | val/loss: 0.34025 | val/accuracy: 0.85774 | val/AUC: 0.85859 | val/Kappa: 0.71591\n",
      "2023-11-30 21:57:45.881 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.302132 to 0.290709. Saving model ...\n",
      "2023-11-30 21:57:46.212 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 13/30 | epoch time: 16.0m 30.18s | lr: 1.00000e-04 | train/loss: 0.26695 | val/loss: 0.29071 | val/accuracy: 0.87460 | val/AUC: 0.87378 | val/Kappa: 0.74874\n",
      "2023-11-30 22:14:13.661 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-30 22:14:13.664 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 14/30 | epoch time: 16.0m 27.44s | lr: 1.00000e-04 | train/loss: 0.26197 | val/loss: 0.29955 | val/accuracy: 0.87250 | val/AUC: 0.87227 | val/Kappa: 0.74482\n",
      "2023-11-30 22:30:08.893 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 2 out of 5\n",
      "2023-11-30 22:30:08.894 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 15/30 | epoch time: 15.0m 55.23s | lr: 1.00000e-04 | train/loss: 0.25214 | val/loss: 0.31336 | val/accuracy: 0.86591 | val/AUC: 0.86572 | val/Kappa: 0.73166\n",
      "2023-11-30 22:46:03.054 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 3 out of 5\n",
      "2023-11-30 22:46:03.056 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 16/30 | epoch time: 15.0m 54.16s | lr: 1.00000e-04 | train/loss: 0.24926 | val/loss: 0.29244 | val/accuracy: 0.88093 | val/AUC: 0.88065 | val/Kappa: 0.76167\n",
      "2023-11-30 23:06:40.251 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 4 out of 5\n",
      "2023-11-30 23:06:40.252 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 17/30 | epoch time: 20.0m 37.19s | lr: 1.00000e-04 | train/loss: 0.24024 | val/loss: 0.29218 | val/accuracy: 0.87987 | val/AUC: 0.88008 | val/Kappa: 0.75979\n",
      "2023-11-30 23:22:40.077 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.290709 to 0.283091. Saving model ...\n",
      "2023-11-30 23:22:40.387 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 18/30 | epoch time: 15.0m 59.82s | lr: 1.00000e-04 | train/loss: 0.23810 | val/loss: 0.28309 | val/accuracy: 0.88330 | val/AUC: 0.88240 | val/Kappa: 0.76612\n",
      "2023-11-30 23:40:24.501 | INFO     | callbacks.early_stopping:save_checkpoint:55 - Valid loss improved from 0.283091 to 0.281840. Saving model ...\n",
      "2023-11-30 23:40:24.795 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 19/30 | epoch time: 17.0m 44.11s | lr: 1.00000e-04 | train/loss: 0.23527 | val/loss: 0.28184 | val/accuracy: 0.88014 | val/AUC: 0.87984 | val/Kappa: 0.76008\n",
      "2023-11-30 23:57:15.608 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 1 out of 5\n",
      "2023-11-30 23:57:15.609 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 20/30 | epoch time: 16.0m 50.81s | lr: 1.00000e-04 | train/loss: 0.23116 | val/loss: 0.28309 | val/accuracy: 0.88040 | val/AUC: 0.87994 | val/Kappa: 0.76053\n",
      "2023-12-01 00:14:33.270 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 2 out of 5\n",
      "2023-12-01 00:14:33.271 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 21/30 | epoch time: 17.0m 17.66s | lr: 1.00000e-04 | train/loss: 0.22428 | val/loss: 0.30116 | val/accuracy: 0.87513 | val/AUC: 0.87514 | val/Kappa: 0.75021\n",
      "2023-12-01 00:31:55.840 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 3 out of 5\n",
      "2023-12-01 00:31:55.841 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 22/30 | epoch time: 17.0m 22.57s | lr: 1.00000e-04 | train/loss: 0.21849 | val/loss: 0.30699 | val/accuracy: 0.87803 | val/AUC: 0.87802 | val/Kappa: 0.75600\n",
      "2023-12-01 00:48:46.976 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 4 out of 5\n",
      "2023-12-01 00:48:46.977 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 23/30 | epoch time: 16.0m 51.13s | lr: 1.00000e-04 | train/loss: 0.21234 | val/loss: 0.28396 | val/accuracy: 0.88672 | val/AUC: 0.88651 | val/Kappa: 0.77330\n",
      "2023-12-01 01:05:52.365 | INFO     | callbacks.early_stopping:__call__:46 - EarlyStopping counter: 5 out of 5\n",
      "2023-12-01 01:05:52.366 | INFO     | experiments.ClassifierExperiment:run:374 - Epoch: 24/30 | epoch time: 17.0m 5.386s | lr: 1.00000e-04 | train/loss: 0.21156 | val/loss: 0.28682 | val/accuracy: 0.88462 | val/AUC: 0.88357 | val/Kappa: 0.76869\n",
      "2023-12-01 01:05:52.368 | WARNING  | experiments.ClassifierExperiment:run:380 - Early stopping triggered at epoch 24. Ending model training.\n",
      "2023-12-01 01:05:52.370 | INFO     | experiments.ClassifierExperiment:run_test:240 - Testing...\n",
      "2023-12-01 01:07:16.521 | INFO     | experiments.ClassifierExperiment:run_test:295 - Test Accuracy: 0.88462\n",
      "2023-12-01 01:07:16.522 | INFO     | experiments.ClassifierExperiment:run_test:296 - Test AUC: 0.88357\n",
      "2023-12-01 01:07:16.523 | INFO     | experiments.ClassifierExperiment:run_test:297 - Test Kappa: 0.76869\n",
      "2023-12-01 01:07:16.523 | INFO     | experiments.ClassifierExperiment:run_test:301 - Target 0 - Precision: 0.84705, Recall: 0.94355, F-score: 0.89270, Support: 1931\n",
      "2023-12-01 01:07:16.524 | INFO     | experiments.ClassifierExperiment:run_test:301 - Target 1 - Precision: 0.93374, Recall: 0.82359, F-score: 0.87521, Support: 1865\n",
      "2023-12-01 01:07:16.524 | INFO     | experiments.ClassifierExperiment:run_test:303 - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      1931\n",
      "           1       0.93      0.82      0.88      1865\n",
      "\n",
      "    accuracy                           0.88      3796\n",
      "   macro avg       0.89      0.88      0.88      3796\n",
      "weighted avg       0.89      0.88      0.88      3796\n",
      "\n",
      "2023-12-01 01:07:16.528 | INFO     | experiments.ClassifierExperiment:run_test:327 - \n",
      "Testing complete.\n",
      "2023-12-01 01:07:16.529 | INFO     | experiments.ClassifierExperiment:run:387 - Run complete. Total time: 07:22:41\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierExperiment\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"30\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"1\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 5 folds CV using StratifiedKFold and averaging all metrics at the end. Also, evaluate all 5 models using majority voting in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting training pipeline with 20 epochs and 5 folds.\n",
      "Loading the data from ../datasets/challenge1/train\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Valid loss improved from inf to 0.386298. Saving model ...\n",
      "[CV 1/5]: Epoch: 01/20 | epoch time: 19.0m 27.83s | lr: 1.00000e-04 | train/loss: 0.44440 | val/loss: 0.38630 | val/accuracy: 0.82653 | val/AUC: 0.82519 | val/Kappa: 0.65207\n",
      "Valid loss improved from 0.386298 to 0.355428. Saving model ...\n",
      "[CV 1/5]: Epoch: 02/20 | epoch time: 19.0m 1.74s | lr: 1.00000e-04 | train/loss: 0.38351 | val/loss: 0.35543 | val/accuracy: 0.83785 | val/AUC: 0.83676 | val/Kappa: 0.67493\n",
      "Valid loss improved from 0.355428 to 0.346949. Saving model ...\n",
      "[CV 1/5]: Epoch: 03/20 | epoch time: 17.0m 58.93s | lr: 1.00000e-04 | train/loss: 0.36290 | val/loss: 0.34695 | val/accuracy: 0.85154 | val/AUC: 0.85048 | val/Kappa: 0.70239\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 1/5]: Epoch: 04/20 | epoch time: 18.0m 27.43s | lr: 1.00000e-04 | train/loss: 0.34908 | val/loss: 0.35908 | val/accuracy: 0.84733 | val/AUC: 0.84672 | val/Kappa: 0.69423\n",
      "Valid loss improved from 0.346949 to 0.322978. Saving model ...\n",
      "[CV 1/5]: Epoch: 05/20 | epoch time: 21.0m 59.0s | lr: 1.00000e-04 | train/loss: 0.33609 | val/loss: 0.32298 | val/accuracy: 0.85759 | val/AUC: 0.85764 | val/Kappa: 0.71515\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 1/5]: Epoch: 06/20 | epoch time: 22.0m 47.75s | lr: 1.00000e-04 | train/loss: 0.32152 | val/loss: 0.32386 | val/accuracy: 0.86102 | val/AUC: 0.85968 | val/Kappa: 0.72123\n",
      "Valid loss improved from 0.322978 to 0.309390. Saving model ...\n",
      "[CV 1/5]: Epoch: 07/20 | epoch time: 18.0m 56.27s | lr: 1.00000e-04 | train/loss: 0.31201 | val/loss: 0.30939 | val/accuracy: 0.87523 | val/AUC: 0.87428 | val/Kappa: 0.74993\n",
      "Valid loss improved from 0.309390 to 0.307570. Saving model ...\n",
      "[CV 1/5]: Epoch: 08/20 | epoch time: 19.0m 24.77s | lr: 1.00000e-04 | train/loss: 0.30243 | val/loss: 0.30757 | val/accuracy: 0.87023 | val/AUC: 0.86937 | val/Kappa: 0.73995\n",
      "Valid loss improved from 0.307570 to 0.307264. Saving model ...\n",
      "[CV 1/5]: Epoch: 09/20 | epoch time: 18.0m 45.85s | lr: 1.00000e-04 | train/loss: 0.29447 | val/loss: 0.30726 | val/accuracy: 0.86733 | val/AUC: 0.86651 | val/Kappa: 0.73417\n",
      "Valid loss improved from 0.307264 to 0.300861. Saving model ...\n",
      "[CV 1/5]: Epoch: 10/20 | epoch time: 18.0m 6.624s | lr: 1.00000e-04 | train/loss: 0.28870 | val/loss: 0.30086 | val/accuracy: 0.87233 | val/AUC: 0.87145 | val/Kappa: 0.74416\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 1/5]: Epoch: 11/20 | epoch time: 18.0m 8.521s | lr: 1.00000e-04 | train/loss: 0.27837 | val/loss: 0.32348 | val/accuracy: 0.86181 | val/AUC: 0.86069 | val/Kappa: 0.72293\n",
      "Valid loss improved from 0.300861 to 0.291329. Saving model ...\n",
      "[CV 1/5]: Epoch: 12/20 | epoch time: 18.0m 1.879s | lr: 1.00000e-04 | train/loss: 0.27375 | val/loss: 0.29133 | val/accuracy: 0.87813 | val/AUC: 0.87807 | val/Kappa: 0.75617\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 1/5]: Epoch: 13/20 | epoch time: 18.0m 26.04s | lr: 1.00000e-04 | train/loss: 0.26467 | val/loss: 0.30720 | val/accuracy: 0.87681 | val/AUC: 0.87605 | val/Kappa: 0.75319\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 1/5]: Epoch: 14/20 | epoch time: 18.0m 46.98s | lr: 1.00000e-04 | train/loss: 0.25940 | val/loss: 0.29154 | val/accuracy: 0.87813 | val/AUC: 0.87855 | val/Kappa: 0.75640\n",
      "Valid loss improved from 0.291329 to 0.265929. Saving model ...\n",
      "[CV 1/5]: Epoch: 15/20 | epoch time: 18.0m 22.21s | lr: 1.00000e-04 | train/loss: 0.25975 | val/loss: 0.26593 | val/accuracy: 0.88760 | val/AUC: 0.88711 | val/Kappa: 0.77493\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 1/5]: Epoch: 16/20 | epoch time: 18.0m 18.13s | lr: 1.00000e-04 | train/loss: 0.24765 | val/loss: 0.28679 | val/accuracy: 0.87918 | val/AUC: 0.87918 | val/Kappa: 0.75831\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 1/5]: Epoch: 17/20 | epoch time: 18.0m 11.62s | lr: 1.00000e-04 | train/loss: 0.24343 | val/loss: 0.30247 | val/accuracy: 0.87444 | val/AUC: 0.87342 | val/Kappa: 0.74831\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 1/5]: Epoch: 18/20 | epoch time: 18.0m 50.02s | lr: 1.00000e-04 | train/loss: 0.23647 | val/loss: 0.29412 | val/accuracy: 0.88049 | val/AUC: 0.88084 | val/Kappa: 0.76110\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[CV 1/5]: Epoch: 19/20 | epoch time: 18.0m 18.99s | lr: 1.00000e-04 | train/loss: 0.23338 | val/loss: 0.29960 | val/accuracy: 0.87813 | val/AUC: 0.87853 | val/Kappa: 0.75639\n",
      "EarlyStopping counter: 5 out of 5\n",
      "[CV 1/5]: Epoch: 20/20 | epoch time: 18.0m 49.99s | lr: 1.00000e-04 | train/loss: 0.22646 | val/loss: 0.28224 | val/accuracy: 0.88734 | val/AUC: 0.88693 | val/Kappa: 0.77444\n",
      "[CV 1/5]: Early stopping triggered at epoch 20. Ending model training.\n",
      "[CV 1/5]: Run complete. Total time: 06:19:13\n",
      "[CV 1/5]: Testing...\n",
      "[CV 1/5]: Test Accuracy: 0.88734\n",
      "[CV 1/5]: Test AUC: 0.88693\n",
      "[CV 1/5]: Test Kappa: 0.77444\n",
      "[CV 1/5]: Target 0 - Precision: 0.87302, Recall: 0.91097, F-score: 0.89159, Sensitivity: 0.9109730848861284, Specificity: 0.8628816282806642, Support: 1932\n",
      "[CV 1/5]: Target 1 - Precision: 0.90353, Recall: 0.86288, F-score: 0.88274, Sensitivity: 0.8628816282806642, Specificity: 0.9109730848861284, Support: 1867\n",
      "[CV 1/5]: Testing complete.\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Valid loss improved from inf to 0.407806. Saving model ...\n",
      "[CV 2/5]: Epoch: 01/20 | epoch time: 19.0m 2.917s | lr: 1.00000e-04 | train/loss: 0.43926 | val/loss: 0.40781 | val/accuracy: 0.82517 | val/AUC: 0.82335 | val/Kappa: 0.64900\n",
      "Valid loss improved from 0.407806 to 0.379041. Saving model ...\n",
      "[CV 2/5]: Epoch: 02/20 | epoch time: 18.0m 43.37s | lr: 1.00000e-04 | train/loss: 0.38844 | val/loss: 0.37904 | val/accuracy: 0.81938 | val/AUC: 0.82075 | val/Kappa: 0.63968\n",
      "Valid loss improved from 0.379041 to 0.365906. Saving model ...\n",
      "[CV 2/5]: Epoch: 03/20 | epoch time: 19.0m 23.97s | lr: 1.00000e-04 | train/loss: 0.36345 | val/loss: 0.36591 | val/accuracy: 0.83149 | val/AUC: 0.83269 | val/Kappa: 0.66373\n",
      "Valid loss improved from 0.365906 to 0.330094. Saving model ...\n",
      "[CV 2/5]: Epoch: 04/20 | epoch time: 18.0m 41.12s | lr: 1.00000e-04 | train/loss: 0.34623 | val/loss: 0.33009 | val/accuracy: 0.85835 | val/AUC: 0.85819 | val/Kappa: 0.71655\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 2/5]: Epoch: 05/20 | epoch time: 18.0m 42.19s | lr: 1.00000e-04 | train/loss: 0.33231 | val/loss: 0.33296 | val/accuracy: 0.85887 | val/AUC: 0.85834 | val/Kappa: 0.71739\n",
      "Valid loss improved from 0.330094 to 0.308763. Saving model ...\n",
      "[CV 2/5]: Epoch: 06/20 | epoch time: 18.0m 24.44s | lr: 1.00000e-04 | train/loss: 0.32441 | val/loss: 0.30876 | val/accuracy: 0.86440 | val/AUC: 0.86424 | val/Kappa: 0.72866\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 2/5]: Epoch: 07/20 | epoch time: 18.0m 41.74s | lr: 1.00000e-04 | train/loss: 0.31200 | val/loss: 0.35868 | val/accuracy: 0.84229 | val/AUC: 0.84040 | val/Kappa: 0.68331\n",
      "Valid loss improved from 0.308763 to 0.301110. Saving model ...\n",
      "[CV 2/5]: Epoch: 08/20 | epoch time: 18.0m 22.32s | lr: 1.00000e-04 | train/loss: 0.30644 | val/loss: 0.30111 | val/accuracy: 0.87493 | val/AUC: 0.87457 | val/Kappa: 0.74964\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 2/5]: Epoch: 09/20 | epoch time: 19.0m 13.07s | lr: 1.00000e-04 | train/loss: 0.30443 | val/loss: 0.32041 | val/accuracy: 0.86203 | val/AUC: 0.86122 | val/Kappa: 0.72356\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 2/5]: Epoch: 10/20 | epoch time: 18.0m 31.67s | lr: 1.00000e-04 | train/loss: 0.29156 | val/loss: 0.30369 | val/accuracy: 0.86704 | val/AUC: 0.86608 | val/Kappa: 0.73351\n",
      "Valid loss improved from 0.301110 to 0.297670. Saving model ...\n",
      "[CV 2/5]: Epoch: 11/20 | epoch time: 18.0m 41.56s | lr: 1.00000e-04 | train/loss: 0.28229 | val/loss: 0.29767 | val/accuracy: 0.87546 | val/AUC: 0.87511 | val/Kappa: 0.75069\n",
      "Valid loss improved from 0.297670 to 0.290510. Saving model ...\n",
      "[CV 2/5]: Epoch: 12/20 | epoch time: 19.0m 50.8s | lr: 1.00000e-04 | train/loss: 0.27735 | val/loss: 0.29051 | val/accuracy: 0.87783 | val/AUC: 0.87690 | val/Kappa: 0.75515\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 2/5]: Epoch: 13/20 | epoch time: 18.0m 54.4s | lr: 1.00000e-04 | train/loss: 0.27208 | val/loss: 0.30136 | val/accuracy: 0.87309 | val/AUC: 0.87229 | val/Kappa: 0.74572\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 2/5]: Epoch: 14/20 | epoch time: 18.0m 33.34s | lr: 1.00000e-04 | train/loss: 0.26970 | val/loss: 0.30212 | val/accuracy: 0.86598 | val/AUC: 0.86639 | val/Kappa: 0.73213\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 2/5]: Epoch: 15/20 | epoch time: 18.0m 43.1s | lr: 1.00000e-04 | train/loss: 0.25745 | val/loss: 0.29089 | val/accuracy: 0.87704 | val/AUC: 0.87617 | val/Kappa: 0.75360\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[CV 2/5]: Epoch: 16/20 | epoch time: 18.0m 17.87s | lr: 1.00000e-04 | train/loss: 0.24935 | val/loss: 0.29949 | val/accuracy: 0.87625 | val/AUC: 0.87517 | val/Kappa: 0.75191\n",
      "EarlyStopping counter: 5 out of 5\n",
      "[CV 2/5]: Epoch: 17/20 | epoch time: 18.0m 15.64s | lr: 1.00000e-04 | train/loss: 0.24321 | val/loss: 0.30116 | val/accuracy: 0.86467 | val/AUC: 0.86438 | val/Kappa: 0.72912\n",
      "[CV 2/5]: Early stopping triggered at epoch 17. Ending model training.\n",
      "[CV 2/5]: Run complete. Total time: 05:19:05\n",
      "[CV 2/5]: Testing...\n",
      "[CV 2/5]: Test Accuracy: 0.86467\n",
      "[CV 2/5]: Test AUC: 0.86438\n",
      "[CV 2/5]: Test Kappa: 0.72912\n",
      "[CV 2/5]: Target 0 - Precision: 0.85657, Recall: 0.88141, F-score: 0.86881, Sensitivity: 0.8814085965820818, Specificity: 0.8473486877343331, Support: 1931\n",
      "[CV 2/5]: Target 1 - Precision: 0.87355, Recall: 0.84735, F-score: 0.86025, Sensitivity: 0.8473486877343331, Specificity: 0.8814085965820818, Support: 1867\n",
      "[CV 2/5]: Testing complete.\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Valid loss improved from inf to 0.417185. Saving model ...\n",
      "[CV 3/5]: Epoch: 01/20 | epoch time: 18.0m 10.98s | lr: 1.00000e-04 | train/loss: 0.44307 | val/loss: 0.41718 | val/accuracy: 0.81174 | val/AUC: 0.81302 | val/Kappa: 0.62438\n",
      "Valid loss improved from 0.417185 to 0.370842. Saving model ...\n",
      "[CV 3/5]: Epoch: 02/20 | epoch time: 18.0m 57.31s | lr: 1.00000e-04 | train/loss: 0.38069 | val/loss: 0.37084 | val/accuracy: 0.83412 | val/AUC: 0.83273 | val/Kappa: 0.66725\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 3/5]: Epoch: 03/20 | epoch time: 18.0m 29.05s | lr: 1.00000e-04 | train/loss: 0.35957 | val/loss: 0.37704 | val/accuracy: 0.83123 | val/AUC: 0.83224 | val/Kappa: 0.66307\n",
      "Valid loss improved from 0.370842 to 0.340753. Saving model ...\n",
      "[CV 3/5]: Epoch: 04/20 | epoch time: 18.0m 22.13s | lr: 1.00000e-04 | train/loss: 0.34172 | val/loss: 0.34075 | val/accuracy: 0.85413 | val/AUC: 0.85310 | val/Kappa: 0.70760\n",
      "Valid loss improved from 0.340753 to 0.339905. Saving model ...\n",
      "[CV 3/5]: Epoch: 05/20 | epoch time: 19.0m 12.72s | lr: 1.00000e-04 | train/loss: 0.33021 | val/loss: 0.33991 | val/accuracy: 0.84834 | val/AUC: 0.84863 | val/Kappa: 0.69680\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 3/5]: Epoch: 06/20 | epoch time: 19.0m 43.19s | lr: 1.00000e-04 | train/loss: 0.32259 | val/loss: 0.37733 | val/accuracy: 0.84308 | val/AUC: 0.84426 | val/Kappa: 0.68683\n",
      "Valid loss improved from 0.339905 to 0.328012. Saving model ...\n",
      "[CV 3/5]: Epoch: 07/20 | epoch time: 19.0m 2.922s | lr: 1.00000e-04 | train/loss: 0.30730 | val/loss: 0.32801 | val/accuracy: 0.86203 | val/AUC: 0.86161 | val/Kappa: 0.72378\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 3/5]: Epoch: 08/20 | epoch time: 18.0m 54.71s | lr: 1.00000e-04 | train/loss: 0.30214 | val/loss: 0.33558 | val/accuracy: 0.86177 | val/AUC: 0.86214 | val/Kappa: 0.72369\n",
      "Valid loss improved from 0.328012 to 0.298132. Saving model ...\n",
      "[CV 3/5]: Epoch: 09/20 | epoch time: 18.0m 56.44s | lr: 1.00000e-04 | train/loss: 0.29178 | val/loss: 0.29813 | val/accuracy: 0.87625 | val/AUC: 0.87546 | val/Kappa: 0.75206\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 3/5]: Epoch: 10/20 | epoch time: 19.0m 40.18s | lr: 1.00000e-04 | train/loss: 0.28355 | val/loss: 0.33720 | val/accuracy: 0.85756 | val/AUC: 0.85638 | val/Kappa: 0.71439\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 3/5]: Epoch: 11/20 | epoch time: 19.0m 1.027s | lr: 1.00000e-04 | train/loss: 0.27811 | val/loss: 0.31188 | val/accuracy: 0.86546 | val/AUC: 0.86446 | val/Kappa: 0.73032\n",
      "Valid loss improved from 0.298132 to 0.291082. Saving model ...\n",
      "[CV 3/5]: Epoch: 12/20 | epoch time: 19.0m 26.6s | lr: 1.00000e-04 | train/loss: 0.27296 | val/loss: 0.29108 | val/accuracy: 0.87704 | val/AUC: 0.87660 | val/Kappa: 0.75381\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 3/5]: Epoch: 13/20 | epoch time: 18.0m 54.85s | lr: 1.00000e-04 | train/loss: 0.25992 | val/loss: 0.32486 | val/accuracy: 0.86519 | val/AUC: 0.86487 | val/Kappa: 0.73015\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 3/5]: Epoch: 14/20 | epoch time: 18.0m 55.64s | lr: 1.00000e-04 | train/loss: 0.26192 | val/loss: 0.31983 | val/accuracy: 0.86440 | val/AUC: 0.86425 | val/Kappa: 0.72867\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 3/5]: Epoch: 15/20 | epoch time: 19.0m 14.71s | lr: 1.00000e-04 | train/loss: 0.25133 | val/loss: 0.30365 | val/accuracy: 0.86993 | val/AUC: 0.86883 | val/Kappa: 0.73923\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[CV 3/5]: Epoch: 16/20 | epoch time: 20.0m 7.288s | lr: 1.00000e-04 | train/loss: 0.24795 | val/loss: 0.32719 | val/accuracy: 0.86756 | val/AUC: 0.86804 | val/Kappa: 0.73532\n",
      "EarlyStopping counter: 5 out of 5\n",
      "[CV 3/5]: Epoch: 17/20 | epoch time: 18.0m 28.74s | lr: 1.00000e-04 | train/loss: 0.23777 | val/loss: 0.29288 | val/accuracy: 0.87915 | val/AUC: 0.87908 | val/Kappa: 0.75821\n",
      "[CV 3/5]: Early stopping triggered at epoch 17. Ending model training.\n",
      "[CV 3/5]: Run complete. Total time: 05:23:40\n",
      "[CV 3/5]: Testing...\n",
      "[CV 3/5]: Test Accuracy: 0.87915\n",
      "[CV 3/5]: Test AUC: 0.87908\n",
      "[CV 3/5]: Test Kappa: 0.75821\n",
      "[CV 3/5]: Target 0 - Precision: 0.87977, Recall: 0.88296, F-score: 0.88136, Sensitivity: 0.8829621957534955, Specificity: 0.8752008569898232, Support: 1931\n",
      "[CV 3/5]: Target 1 - Precision: 0.87849, Recall: 0.87520, F-score: 0.87684, Sensitivity: 0.8752008569898232, Specificity: 0.8829621957534955, Support: 1867\n",
      "[CV 3/5]: Testing complete.\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Valid loss improved from inf to 0.365809. Saving model ...\n",
      "[CV 4/5]: Epoch: 01/20 | epoch time: 17.0m 59.65s | lr: 1.00000e-04 | train/loss: 0.45151 | val/loss: 0.36581 | val/accuracy: 0.83702 | val/AUC: 0.83608 | val/Kappa: 0.67336\n",
      "Valid loss improved from 0.365809 to 0.337315. Saving model ...\n",
      "[CV 4/5]: Epoch: 02/20 | epoch time: 18.0m 6.593s | lr: 1.00000e-04 | train/loss: 0.38318 | val/loss: 0.33732 | val/accuracy: 0.85071 | val/AUC: 0.84991 | val/Kappa: 0.70088\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 4/5]: Epoch: 03/20 | epoch time: 18.0m 32.48s | lr: 1.00000e-04 | train/loss: 0.36545 | val/loss: 0.36164 | val/accuracy: 0.84360 | val/AUC: 0.84216 | val/Kappa: 0.68623\n",
      "Valid loss improved from 0.337315 to 0.309167. Saving model ...\n",
      "[CV 4/5]: Epoch: 04/20 | epoch time: 18.0m 53.35s | lr: 1.00000e-04 | train/loss: 0.34649 | val/loss: 0.30917 | val/accuracy: 0.86704 | val/AUC: 0.86714 | val/Kappa: 0.73407\n",
      "Valid loss improved from 0.309167 to 0.308607. Saving model ...\n",
      "[CV 4/5]: Epoch: 05/20 | epoch time: 18.0m 19.78s | lr: 1.00000e-04 | train/loss: 0.33680 | val/loss: 0.30861 | val/accuracy: 0.86203 | val/AUC: 0.86079 | val/Kappa: 0.72332\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 4/5]: Epoch: 06/20 | epoch time: 18.0m 11.44s | lr: 1.00000e-04 | train/loss: 0.32787 | val/loss: 0.32563 | val/accuracy: 0.85519 | val/AUC: 0.85357 | val/Kappa: 0.70938\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 4/5]: Epoch: 07/20 | epoch time: 18.0m 47.15s | lr: 1.00000e-04 | train/loss: 0.31423 | val/loss: 0.31772 | val/accuracy: 0.87177 | val/AUC: 0.87205 | val/Kappa: 0.74364\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 4/5]: Epoch: 08/20 | epoch time: 19.0m 7.954s | lr: 1.00000e-04 | train/loss: 0.30787 | val/loss: 0.32328 | val/accuracy: 0.85966 | val/AUC: 0.85848 | val/Kappa: 0.71860\n",
      "Valid loss improved from 0.308607 to 0.281302. Saving model ...\n",
      "[CV 4/5]: Epoch: 09/20 | epoch time: 18.0m 41.46s | lr: 1.00000e-04 | train/loss: 0.29717 | val/loss: 0.28130 | val/accuracy: 0.87941 | val/AUC: 0.87906 | val/Kappa: 0.75860\n",
      "Valid loss improved from 0.281302 to 0.279030. Saving model ...\n",
      "[CV 4/5]: Epoch: 10/20 | epoch time: 19.0m 1.9s | lr: 1.00000e-04 | train/loss: 0.29055 | val/loss: 0.27903 | val/accuracy: 0.87888 | val/AUC: 0.87868 | val/Kappa: 0.75762\n",
      "Valid loss improved from 0.279030 to 0.274702. Saving model ...\n",
      "[CV 4/5]: Epoch: 11/20 | epoch time: 19.0m 19.34s | lr: 1.00000e-04 | train/loss: 0.28417 | val/loss: 0.27470 | val/accuracy: 0.88389 | val/AUC: 0.88322 | val/Kappa: 0.76741\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 4/5]: Epoch: 12/20 | epoch time: 19.0m 16.35s | lr: 1.00000e-04 | train/loss: 0.27239 | val/loss: 0.31294 | val/accuracy: 0.86704 | val/AUC: 0.86769 | val/Kappa: 0.73437\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 4/5]: Epoch: 13/20 | epoch time: 19.0m 32.44s | lr: 1.00000e-04 | train/loss: 0.26847 | val/loss: 0.28861 | val/accuracy: 0.88020 | val/AUC: 0.87933 | val/Kappa: 0.75993\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 4/5]: Epoch: 14/20 | epoch time: 19.0m 48.2s | lr: 1.00000e-04 | train/loss: 0.26334 | val/loss: 0.30054 | val/accuracy: 0.88231 | val/AUC: 0.88263 | val/Kappa: 0.76472\n",
      "Valid loss improved from 0.274702 to 0.269703. Saving model ...\n",
      "[CV 4/5]: Epoch: 15/20 | epoch time: 20.0m 9.76s | lr: 1.00000e-04 | train/loss: 0.25849 | val/loss: 0.26970 | val/accuracy: 0.88547 | val/AUC: 0.88475 | val/Kappa: 0.77055\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 4/5]: Epoch: 16/20 | epoch time: 19.0m 24.44s | lr: 1.00000e-04 | train/loss: 0.24671 | val/loss: 0.31343 | val/accuracy: 0.86493 | val/AUC: 0.86385 | val/Kappa: 0.72922\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 4/5]: Epoch: 17/20 | epoch time: 19.0m 31.18s | lr: 1.00000e-04 | train/loss: 0.25115 | val/loss: 0.29913 | val/accuracy: 0.87388 | val/AUC: 0.87328 | val/Kappa: 0.74741\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 4/5]: Epoch: 18/20 | epoch time: 19.0m 6.411s | lr: 1.00000e-04 | train/loss: 0.24073 | val/loss: 0.32460 | val/accuracy: 0.85545 | val/AUC: 0.85363 | val/Kappa: 0.70979\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[CV 4/5]: Epoch: 19/20 | epoch time: 19.0m 23.32s | lr: 1.00000e-04 | train/loss: 0.23389 | val/loss: 0.29313 | val/accuracy: 0.87572 | val/AUC: 0.87496 | val/Kappa: 0.75102\n",
      "Valid loss improved from 0.269703 to 0.257794. Saving model ...\n",
      "[CV 4/5]: Epoch: 20/20 | epoch time: 19.0m 54.28s | lr: 1.00000e-04 | train/loss: 0.22674 | val/loss: 0.25779 | val/accuracy: 0.89336 | val/AUC: 0.89350 | val/Kappa: 0.78674\n",
      "[CV 4/5]: Run complete. Total time: 06:21:10\n",
      "[CV 4/5]: Testing...\n",
      "[CV 4/5]: Test Accuracy: 0.89336\n",
      "[CV 4/5]: Test AUC: 0.89350\n",
      "[CV 4/5]: Test Kappa: 0.78674\n",
      "[CV 4/5]: Target 0 - Precision: 0.90285, Recall: 0.88555, F-score: 0.89412, Sensitivity: 0.8855515277058519, Specificity: 0.9014461703267274, Support: 1931\n",
      "[CV 4/5]: Target 1 - Precision: 0.88393, Recall: 0.90145, F-score: 0.89260, Sensitivity: 0.9014461703267274, Specificity: 0.8855515277058519, Support: 1867\n",
      "[CV 4/5]: Testing complete.\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Valid loss improved from inf to 0.373322. Saving model ...\n",
      "[CV 5/5]: Epoch: 01/20 | epoch time: 19.0m 44.77s | lr: 1.00000e-04 | train/loss: 0.44567 | val/loss: 0.37332 | val/accuracy: 0.82807 | val/AUC: 0.82676 | val/Kappa: 0.65517\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 5/5]: Epoch: 02/20 | epoch time: 20.0m 56.95s | lr: 1.00000e-04 | train/loss: 0.38685 | val/loss: 0.38384 | val/accuracy: 0.83149 | val/AUC: 0.83260 | val/Kappa: 0.66366\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 5/5]: Epoch: 03/20 | epoch time: 18.0m 57.06s | lr: 1.00000e-04 | train/loss: 0.36376 | val/loss: 0.41458 | val/accuracy: 0.80885 | val/AUC: 0.81082 | val/Kappa: 0.61913\n",
      "Valid loss improved from 0.373322 to 0.319100. Saving model ...\n",
      "[CV 5/5]: Epoch: 04/20 | epoch time: 22.0m 25.68s | lr: 1.00000e-04 | train/loss: 0.34779 | val/loss: 0.31910 | val/accuracy: 0.86467 | val/AUC: 0.86431 | val/Kappa: 0.72908\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 5/5]: Epoch: 05/20 | epoch time: 20.0m 53.41s | lr: 1.00000e-04 | train/loss: 0.33921 | val/loss: 0.33213 | val/accuracy: 0.85124 | val/AUC: 0.84996 | val/Kappa: 0.70165\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 5/5]: Epoch: 06/20 | epoch time: 19.0m 31.67s | lr: 1.00000e-04 | train/loss: 0.32480 | val/loss: 0.34452 | val/accuracy: 0.85097 | val/AUC: 0.84943 | val/Kappa: 0.70097\n",
      "Valid loss improved from 0.319100 to 0.311405. Saving model ...\n",
      "[CV 5/5]: Epoch: 07/20 | epoch time: 20.0m 46.31s | lr: 1.00000e-04 | train/loss: 0.31666 | val/loss: 0.31140 | val/accuracy: 0.86862 | val/AUC: 0.86811 | val/Kappa: 0.73691\n",
      "Valid loss improved from 0.311405 to 0.302091. Saving model ...\n",
      "[CV 5/5]: Epoch: 08/20 | epoch time: 21.0m 6.987s | lr: 1.00000e-04 | train/loss: 0.31128 | val/loss: 0.30209 | val/accuracy: 0.86493 | val/AUC: 0.86480 | val/Kappa: 0.72973\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 5/5]: Epoch: 09/20 | epoch time: 20.0m 10.38s | lr: 1.00000e-04 | train/loss: 0.30307 | val/loss: 0.31128 | val/accuracy: 0.85413 | val/AUC: 0.85282 | val/Kappa: 0.70744\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 5/5]: Epoch: 10/20 | epoch time: 19.0m 33.58s | lr: 1.00000e-04 | train/loss: 0.29533 | val/loss: 0.30225 | val/accuracy: 0.86704 | val/AUC: 0.86721 | val/Kappa: 0.73411\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 5/5]: Epoch: 11/20 | epoch time: 19.0m 50.73s | lr: 1.00000e-04 | train/loss: 0.28081 | val/loss: 0.32630 | val/accuracy: 0.86072 | val/AUC: 0.86100 | val/Kappa: 0.72153\n",
      "Valid loss improved from 0.302091 to 0.285027. Saving model ...\n",
      "[CV 5/5]: Epoch: 12/20 | epoch time: 19.0m 37.85s | lr: 1.00000e-04 | train/loss: 0.28272 | val/loss: 0.28503 | val/accuracy: 0.87809 | val/AUC: 0.87742 | val/Kappa: 0.75581\n",
      "Valid loss improved from 0.285027 to 0.275688. Saving model ...\n",
      "[CV 5/5]: Epoch: 13/20 | epoch time: 20.0m 20.47s | lr: 1.00000e-04 | train/loss: 0.27404 | val/loss: 0.27569 | val/accuracy: 0.88152 | val/AUC: 0.88118 | val/Kappa: 0.76282\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[CV 5/5]: Epoch: 14/20 | epoch time: 19.0m 45.5s | lr: 1.00000e-04 | train/loss: 0.26563 | val/loss: 0.29481 | val/accuracy: 0.87441 | val/AUC: 0.87419 | val/Kappa: 0.74865\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[CV 5/5]: Epoch: 15/20 | epoch time: 19.0m 45.78s | lr: 1.00000e-04 | train/loss: 0.25825 | val/loss: 0.31536 | val/accuracy: 0.86546 | val/AUC: 0.86396 | val/Kappa: 0.73005\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[CV 5/5]: Epoch: 16/20 | epoch time: 20.0m 27.87s | lr: 1.00000e-04 | train/loss: 0.24885 | val/loss: 0.29765 | val/accuracy: 0.86783 | val/AUC: 0.86828 | val/Kappa: 0.73584\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[CV 5/5]: Epoch: 17/20 | epoch time: 19.0m 37.42s | lr: 1.00000e-04 | train/loss: 0.24749 | val/loss: 0.29993 | val/accuracy: 0.86888 | val/AUC: 0.86905 | val/Kappa: 0.73779\n",
      "EarlyStopping counter: 5 out of 5\n",
      "[CV 5/5]: Epoch: 18/20 | epoch time: 19.0m 42.45s | lr: 1.00000e-04 | train/loss: 0.24298 | val/loss: 0.28696 | val/accuracy: 0.87783 | val/AUC: 0.87799 | val/Kappa: 0.75569\n",
      "[CV 5/5]: Early stopping triggered at epoch 18. Ending model training.\n",
      "[CV 5/5]: Run complete. Total time: 06:03:16\n",
      "[CV 5/5]: Testing...\n",
      "[CV 5/5]: Test Accuracy: 0.87783\n",
      "[CV 5/5]: Test AUC: 0.87799\n",
      "[CV 5/5]: Test Kappa: 0.75569\n",
      "[CV 5/5]: Target 0 - Precision: 0.88871, Recall: 0.86846, F-score: 0.87847, Sensitivity: 0.8684619368203004, Specificity: 0.8875200856989823, Support: 1931\n",
      "[CV 5/5]: Target 1 - Precision: 0.86709, Recall: 0.88752, F-score: 0.87718, Sensitivity: 0.8875200856989823, Specificity: 0.8684619368203004, Support: 1867\n",
      "[CV 5/5]: Testing complete.\n",
      "Average accuracy: 0.88047. Average AUC: 0.88038. Average Kappa: 0.76084\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train_cv.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierExperimentCV\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"20\" \\\n",
    "                        --num_folds \"5\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"1\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference on the previous experiment using the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting inference script...\n",
      "Constructed output path: outputs/ClassifierExperimentCV_224_epo20_bs32_lr0.0001_s42/2023-12-03_2157_VGG16_BN_Attention. Searching for models...\n",
      "Found 5 models. Starting loading the models.\n",
      "Loading data with 2 class labels from ../datasets/challenge1/val path...\n",
      "Dataset labels: {'nevus': 0, 'others': 1} dictionary.\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Dataset length: 3796\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Namespace(test_path='../datasets/challenge1/val', output='outputs', experiment_name='ClassifierExperimentCV', network_name='VGG16_BN_Attention', max_epochs=20, batch_size=32, base_lr=0.0001, img_size=224, seed=42, timeframe='2023-12-03_2157', verbose=2, multi=False, report=True)\n",
      "\n",
      "Inference:   0%|          | 0/119 [00:00<?, ?it/s]\n",
      "Inference:   1%|          | 1/119 [00:17<34:05, 17.33s/it]\n",
      "Inference:   2%|▏         | 2/119 [00:34<33:17, 17.07s/it]\n",
      "Inference:   3%|▎         | 3/119 [00:50<31:56, 16.52s/it]\n",
      "Inference:   3%|▎         | 4/119 [01:05<31:00, 16.18s/it]\n",
      "Inference:   4%|▍         | 5/119 [01:21<30:32, 16.07s/it]\n",
      "Inference:   5%|▌         | 6/119 [01:37<30:01, 15.95s/it]\n",
      "Inference:   6%|▌         | 7/119 [01:53<29:44, 15.93s/it]\n",
      "Inference:   7%|▋         | 8/119 [02:09<29:32, 15.96s/it]\n",
      "Inference:   8%|▊         | 9/119 [02:25<29:18, 15.99s/it]\n",
      "Inference:   8%|▊         | 10/119 [02:41<28:58, 15.95s/it]\n",
      "Inference:   9%|▉         | 11/119 [02:57<28:42, 15.95s/it]\n",
      "Inference:  10%|█         | 12/119 [03:12<28:18, 15.88s/it]\n",
      "Inference:  11%|█         | 13/119 [03:28<27:51, 15.77s/it]\n",
      "Inference:  12%|█▏        | 14/119 [03:44<27:46, 15.87s/it]\n",
      "Inference:  13%|█▎        | 15/119 [04:00<27:32, 15.89s/it]\n",
      "Inference:  13%|█▎        | 16/119 [04:16<27:11, 15.84s/it]\n",
      "Inference:  14%|█▍        | 17/119 [04:32<27:04, 15.93s/it]\n",
      "Inference:  15%|█▌        | 18/119 [04:48<26:44, 15.89s/it]\n",
      "Inference:  16%|█▌        | 19/119 [05:03<26:21, 15.81s/it]\n",
      "Inference:  17%|█▋        | 20/119 [05:19<26:03, 15.79s/it]\n",
      "Inference:  18%|█▊        | 21/119 [05:35<25:47, 15.79s/it]\n",
      "Inference:  18%|█▊        | 22/119 [05:50<25:29, 15.77s/it]\n",
      "Inference:  19%|█▉        | 23/119 [06:06<25:13, 15.77s/it]\n",
      "Inference:  20%|██        | 24/119 [06:22<25:04, 15.84s/it]\n",
      "Inference:  21%|██        | 25/119 [06:38<24:48, 15.84s/it]\n",
      "Inference:  22%|██▏       | 26/119 [06:54<24:32, 15.83s/it]\n",
      "Inference:  23%|██▎       | 27/119 [07:10<24:38, 16.07s/it]\n",
      "Inference:  24%|██▎       | 28/119 [07:26<24:17, 16.02s/it]\n",
      "Inference:  24%|██▍       | 29/119 [07:42<24:01, 16.02s/it]\n",
      "Inference:  25%|██▌       | 30/119 [07:59<23:54, 16.12s/it]\n",
      "Inference:  26%|██▌       | 31/119 [08:15<23:38, 16.12s/it]\n",
      "Inference:  27%|██▋       | 32/119 [08:31<23:14, 16.03s/it]\n",
      "Inference:  28%|██▊       | 33/119 [08:47<22:56, 16.01s/it]\n",
      "Inference:  29%|██▊       | 34/119 [09:02<22:31, 15.90s/it]\n",
      "Inference:  29%|██▉       | 35/119 [09:18<22:13, 15.87s/it]\n",
      "Inference:  30%|███       | 36/119 [09:34<21:53, 15.83s/it]\n",
      "Inference:  31%|███       | 37/119 [09:50<21:38, 15.83s/it]\n",
      "Inference:  32%|███▏      | 38/119 [10:05<21:18, 15.78s/it]\n",
      "Inference:  33%|███▎      | 39/119 [10:21<21:06, 15.83s/it]\n",
      "Inference:  34%|███▎      | 40/119 [10:37<20:49, 15.82s/it]\n",
      "Inference:  34%|███▍      | 41/119 [10:53<20:37, 15.87s/it]\n",
      "Inference:  35%|███▌      | 42/119 [11:09<20:16, 15.81s/it]\n",
      "Inference:  36%|███▌      | 43/119 [11:25<20:06, 15.88s/it]\n",
      "Inference:  37%|███▋      | 44/119 [11:41<19:49, 15.86s/it]\n",
      "Inference:  38%|███▊      | 45/119 [11:56<19:33, 15.86s/it]\n",
      "Inference:  39%|███▊      | 46/119 [12:12<19:13, 15.81s/it]\n",
      "Inference:  39%|███▉      | 47/119 [12:28<18:56, 15.79s/it]\n",
      "Inference:  40%|████      | 48/119 [12:44<18:43, 15.83s/it]\n",
      "Inference:  41%|████      | 49/119 [13:00<18:30, 15.87s/it]\n",
      "Inference:  42%|████▏     | 50/119 [13:16<18:20, 15.94s/it]\n",
      "Inference:  43%|████▎     | 51/119 [13:32<18:02, 15.92s/it]\n",
      "Inference:  44%|████▎     | 52/119 [13:48<18:01, 16.14s/it]\n",
      "Inference:  45%|████▍     | 53/119 [14:05<17:58, 16.34s/it]\n",
      "Inference:  45%|████▌     | 54/119 [14:22<17:56, 16.56s/it]\n",
      "Inference:  46%|████▌     | 55/119 [14:38<17:30, 16.41s/it]\n",
      "Inference:  47%|████▋     | 56/119 [14:55<17:24, 16.58s/it]\n",
      "Inference:  48%|████▊     | 57/119 [15:13<17:20, 16.78s/it]\n",
      "Inference:  49%|████▊     | 58/119 [15:28<16:48, 16.53s/it]\n",
      "Inference:  50%|████▉     | 59/119 [15:46<16:42, 16.71s/it]\n",
      "Inference:  50%|█████     | 60/119 [16:02<16:24, 16.68s/it]\n",
      "Inference:  51%|█████▏    | 61/119 [16:18<15:58, 16.52s/it]\n",
      "Inference:  52%|█████▏    | 62/119 [16:35<15:40, 16.50s/it]\n",
      "Inference:  53%|█████▎    | 63/119 [16:51<15:22, 16.47s/it]\n",
      "Inference:  54%|█████▍    | 64/119 [17:08<15:18, 16.70s/it]\n",
      "Inference:  55%|█████▍    | 65/119 [17:25<15:05, 16.77s/it]\n",
      "Inference:  55%|█████▌    | 66/119 [17:42<14:48, 16.76s/it]\n",
      "Inference:  56%|█████▋    | 67/119 [17:59<14:34, 16.82s/it]\n",
      "Inference:  57%|█████▋    | 68/119 [18:16<14:26, 16.99s/it]\n",
      "Inference:  58%|█████▊    | 69/119 [18:33<14:10, 17.00s/it]\n",
      "Inference:  59%|█████▉    | 70/119 [18:50<13:48, 16.90s/it]\n",
      "Inference:  60%|█████▉    | 71/119 [19:07<13:28, 16.84s/it]\n",
      "Inference:  61%|██████    | 72/119 [19:23<13:06, 16.73s/it]\n",
      "Inference:  61%|██████▏   | 73/119 [19:40<12:50, 16.75s/it]\n",
      "Inference:  62%|██████▏   | 74/119 [19:57<12:39, 16.89s/it]\n",
      "Inference:  63%|██████▎   | 75/119 [20:14<12:15, 16.71s/it]\n",
      "Inference:  64%|██████▍   | 76/119 [20:30<11:58, 16.71s/it]\n",
      "Inference:  65%|██████▍   | 77/119 [20:47<11:40, 16.68s/it]\n",
      "Inference:  66%|██████▌   | 78/119 [21:04<11:24, 16.70s/it]\n",
      "Inference:  66%|██████▋   | 79/119 [21:20<11:00, 16.51s/it]\n",
      "Inference:  67%|██████▋   | 80/119 [21:36<10:37, 16.35s/it]\n",
      "Inference:  68%|██████▊   | 81/119 [21:52<10:20, 16.33s/it]\n",
      "Inference:  69%|██████▉   | 82/119 [22:08<09:55, 16.10s/it]\n",
      "Inference:  70%|██████▉   | 83/119 [22:23<09:34, 15.97s/it]\n",
      "Inference:  71%|███████   | 84/119 [22:39<09:16, 15.91s/it]\n",
      "Inference:  71%|███████▏  | 85/119 [22:56<09:16, 16.37s/it]\n",
      "Inference:  72%|███████▏  | 86/119 [23:15<09:20, 16.99s/it]\n",
      "Inference:  73%|███████▎  | 87/119 [23:32<09:01, 16.93s/it]\n",
      "Inference:  74%|███████▍  | 88/119 [23:49<08:45, 16.94s/it]\n",
      "Inference:  75%|███████▍  | 89/119 [24:05<08:26, 16.88s/it]\n",
      "Inference:  76%|███████▌  | 90/119 [24:22<08:06, 16.77s/it]\n",
      "Inference:  76%|███████▋  | 91/119 [24:38<07:45, 16.63s/it]\n",
      "Inference:  77%|███████▋  | 92/119 [24:55<07:27, 16.58s/it]\n",
      "Inference:  78%|███████▊  | 93/119 [25:11<07:10, 16.56s/it]\n",
      "Inference:  79%|███████▉  | 94/119 [25:28<06:57, 16.69s/it]\n",
      "Inference:  80%|███████▉  | 95/119 [25:46<06:48, 17.03s/it]\n",
      "Inference:  81%|████████  | 96/119 [26:03<06:33, 17.11s/it]\n",
      "Inference:  82%|████████▏ | 97/119 [26:20<06:16, 17.13s/it]\n",
      "Inference:  82%|████████▏ | 98/119 [26:37<05:54, 16.89s/it]\n",
      "Inference:  83%|████████▎ | 99/119 [26:53<05:33, 16.69s/it]\n",
      "Inference:  84%|████████▍ | 100/119 [27:10<05:16, 16.68s/it]\n",
      "Inference:  85%|████████▍ | 101/119 [27:27<05:01, 16.75s/it]\n",
      "Inference:  86%|████████▌ | 102/119 [27:44<04:46, 16.88s/it]\n",
      "Inference:  87%|████████▋ | 103/119 [28:01<04:29, 16.87s/it]\n",
      "Inference:  87%|████████▋ | 104/119 [28:17<04:11, 16.78s/it]\n",
      "Inference:  88%|████████▊ | 105/119 [28:33<03:52, 16.59s/it]\n",
      "Inference:  89%|████████▉ | 106/119 [28:50<03:35, 16.58s/it]\n",
      "Inference:  90%|████████▉ | 107/119 [29:06<03:18, 16.52s/it]\n",
      "Inference:  91%|█████████ | 108/119 [29:24<03:04, 16.75s/it]\n",
      "Inference:  92%|█████████▏| 109/119 [29:41<02:48, 16.80s/it]\n",
      "Inference:  92%|█████████▏| 110/119 [29:58<02:31, 16.88s/it]\n",
      "Inference:  93%|█████████▎| 111/119 [30:14<02:13, 16.67s/it]\n",
      "Inference:  94%|█████████▍| 112/119 [30:30<01:56, 16.64s/it]\n",
      "Inference:  95%|█████████▍| 113/119 [30:47<01:39, 16.60s/it]\n",
      "Inference:  96%|█████████▌| 114/119 [31:03<01:21, 16.38s/it]\n",
      "Inference:  97%|█████████▋| 115/119 [31:18<01:04, 16.19s/it]\n",
      "Inference:  97%|█████████▋| 116/119 [31:34<00:48, 16.13s/it]\n",
      "Inference:  98%|█████████▊| 117/119 [31:50<00:32, 16.05s/it]\n",
      "Inference:  99%|█████████▉| 118/119 [32:06<00:15, 15.98s/it]\n",
      "Inference: 100%|██████████| 119/119 [32:16<00:00, 14.20s/it]\n",
      "Inference: 100%|██████████| 119/119 [32:16<00:00, 16.28s/it]\n",
      "Results exported to: outputs/ClassifierExperimentCV_224_epo20_bs32_lr0.0001_s42/2023-12-03_2157_VGG16_BN_Attention/ClassifierExperimentCV_224_epo20_bs32_lr0.0001_s42_2023-12-03_2157_VGG16_BN_Attention.csv\n",
      "Majority vote AUC: 0.92256\n",
      "Majority vote accuracy: 0.9230769230769231\n",
      "Majority vote kappa: 0.8459548756987352\n",
      "Target 0 - Precision: 0.90191, Recall: 0.95236, F-score: 0.92645, Sensitivity: 0.9523562920766442, Specificity: 0.8927613941018767, Support: 1931\n",
      "Target 1 - Precision: 0.94764, Recall: 0.89276, F-score: 0.91938, Sensitivity: 0.8927613941018767, Specificity: 0.9523562920766442, Support: 1865\n",
      "Figure(1500x600)\n",
      "(   {   'accuracy': 0.9230769230769231,\n",
      "        'auc': 0.9225588430892604,\n",
      "        'fscore': [0.926448362720403, 0.9193815571507454],\n",
      "        'kappa': 0.8459548756987352,\n",
      "        'precision': [0.9019127023050515, 0.9476380193511668],\n",
      "        'recall': [0.9523562920766442, 0.8927613941018767],\n",
      "        'sensitivity': [0.9523562920766442, 0.8927613941018767],\n",
      "        'specificity': [0.8927613941018767, 0.9523562920766442],\n",
      "        'support': [1931, 1865]},)\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../inference.py --test_path \"../datasets/challenge1/val\" \\\n",
    "                    --experiment_name \"ClassifierExperimentCV\" \\\n",
    "                    --network_name \"VGG16_BN_Attention\" \\\n",
    "                    --max_epochs \"20\" \\\n",
    "                    --batch_size \"32\" \\\n",
    "                    --timeframe \"2023-12-03_2157\" \\\n",
    "                    --verbose \"2\" \\\n",
    "                    --ensemble \\\n",
    "                    --report'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated get_class_weight to use class get_class_weight. Using the training masks and attention masks during the training in `ClassifierSegExperiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting training pipeline with 30 epochs...\n",
      "Loading data with 2 class labels...\n",
      "Loading the data from ../datasets/challenge1/train\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Using segmentation masks for training...\n",
      "train_images: 15195, train_masks: 15195, train_labels: 15195\n",
      "Class weights: [0.98349515 1.01706827]\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using cross entropy loss.\n",
      "Using dice loss.\n",
      "Experiment started.\n",
      "Epoch: 1 Train batch 10 loss: 0.6887211203575134, 2.1% complete\n",
      "Epoch: 1 Train batch 20 loss: 0.4921400547027588, 4.2% complete\n",
      "Epoch: 1 Train batch 30 loss: 0.4343116879463196, 6.3% complete\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --train_masks_path \"../datasets_masks/challenge1/train\" \\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierSegExperiment\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"30\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"2\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Results was run on the server process and saved in the txt file. \n",
    "    \n",
    "    2023-12-20 11:57:14.194 | INFO     | experiments.ClassifierSegExperiment:run_test:387 - Test Accuracy: 0.87460\n",
    "    2023-12-20 11:57:14.194 | INFO     | experiments.ClassifierSegExperiment:run_test:388 - Test AUC: 0.87509\n",
    "    2023-12-20 11:57:14.195 | INFO     | experiments.ClassifierSegExperiment:run_test:390 - Test Kappa: 0.74940\n",
    "    2023-12-20 11:57:14.195 | INFO     | experiments.ClassifierSegExperiment:run_test:394 - Target 0 - Precision: 0.90039, Recall: 0.84723, F-score: 0.87300, Sensitivity: 0.8472294148109788, Specificity: 0.9029490616621983, Support: 1931\n",
    "    2023-12-20 11:57:14.195 | INFO     | experiments.ClassifierSegExperiment:run_test:394 - Target 1 - Precision: 0.85093, Recall: 0.90295, F-score: 0.87617, Sensitivity: 0.9029490616621983, Specificity: 0.8472294148109788, Support: 1865\n",
    "    \n",
    "    2023-12-20 11:57:14.195 | INFO     | experiments.ClassifierSegExperiment:run_test:396 - \n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.90      0.85      0.87      1931\n",
    "               1       0.85      0.90      0.88      1865\n",
    "    \n",
    "        accuracy                           0.87      3796\n",
    "       macro avg       0.88      0.88      0.87      3796\n",
    "    weighted avg       0.88      0.87      0.87      3796\n",
    "    \n",
    "</p>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting inference script...\n",
      "Constructed output path: outputs/ClassifierSegExperiment_224_epo30_bs32_lr0.0001_s42/2023-12-19_2344_VGG16_BN_Attention. Searching for models...\n",
      "Found 1 models. Starting loading the models.\n",
      "Loading data with 2 class labels from ../datasets/challenge1/val path...\n",
      "Dataset labels: {'nevus': 0, 'others': 1} dictionary.\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Dataset length: 3796\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Namespace(test_path='../datasets/challenge1/val', output='outputs', experiment_name='ClassifierSegExperiment', network_name='VGG16_BN_Attention', max_epochs=30, batch_size=32, base_lr=0.0001, img_size=224, seed=42, timeframe='2023-12-19_2344', verbose=2, multi=False, report=True, ensemble=False)\n",
      "\n",
      "Inference:   0%|          | 0/119 [00:00<?, ?it/s]\n",
      "Inference:   1%|          | 1/119 [00:05<10:30,  5.35s/it]\n",
      "Inference:   2%|▏         | 2/119 [00:05<04:28,  2.30s/it]\n",
      "Inference:   3%|▎         | 3/119 [00:05<02:33,  1.32s/it]\n",
      "Inference:   3%|▎         | 4/119 [00:05<01:39,  1.16it/s]\n",
      "Inference:   4%|▍         | 5/119 [00:05<01:09,  1.63it/s]\n",
      "Inference:   5%|▌         | 6/119 [00:06<00:51,  2.19it/s]\n",
      "Inference:   6%|▌         | 7/119 [00:06<00:40,  2.77it/s]\n",
      "Inference:   7%|▋         | 8/119 [00:06<00:33,  3.36it/s]\n",
      "Inference:   8%|▊         | 9/119 [00:06<00:28,  3.91it/s]\n",
      "Inference:   8%|▊         | 10/119 [00:06<00:24,  4.41it/s]\n",
      "Inference:   9%|▉         | 11/119 [00:06<00:22,  4.81it/s]\n",
      "Inference:  10%|█         | 12/119 [00:07<00:23,  4.60it/s]\n",
      "Inference:  11%|█         | 13/119 [00:07<00:23,  4.45it/s]\n",
      "Inference:  12%|█▏        | 14/119 [00:07<00:21,  4.87it/s]\n",
      "Inference:  13%|█▎        | 15/119 [00:08<00:49,  2.10it/s]\n",
      "Inference:  13%|█▎        | 16/119 [00:08<00:39,  2.61it/s]\n",
      "Inference:  14%|█▍        | 17/119 [00:09<00:32,  3.15it/s]\n",
      "Inference:  15%|█▌        | 18/119 [00:09<00:27,  3.69it/s]\n",
      "Inference:  16%|█▌        | 19/119 [00:09<00:37,  2.67it/s]\n",
      "Inference:  17%|█▋        | 20/119 [00:09<00:30,  3.21it/s]\n",
      "Inference:  18%|█▊        | 21/119 [00:10<00:26,  3.75it/s]\n",
      "Inference:  18%|█▊        | 22/119 [00:10<00:22,  4.25it/s]\n",
      "Inference:  19%|█▉        | 23/119 [00:10<00:34,  2.79it/s]\n",
      "Inference:  20%|██        | 24/119 [00:11<00:36,  2.61it/s]\n",
      "Inference:  21%|██        | 25/119 [00:11<00:29,  3.16it/s]\n",
      "Inference:  22%|██▏       | 26/119 [00:11<00:25,  3.71it/s]\n",
      "Inference:  23%|██▎       | 27/119 [00:12<00:34,  2.65it/s]\n",
      "Inference:  24%|██▎       | 28/119 [00:12<00:38,  2.39it/s]\n",
      "Inference:  24%|██▍       | 29/119 [00:13<00:30,  2.93it/s]\n",
      "Inference:  25%|██▌       | 30/119 [00:13<00:25,  3.47it/s]\n",
      "Inference:  26%|██▌       | 31/119 [00:13<00:27,  3.20it/s]\n",
      "Inference:  27%|██▋       | 32/119 [00:14<00:36,  2.40it/s]\n",
      "Inference:  28%|██▊       | 33/119 [00:14<00:29,  2.94it/s]\n",
      "Inference:  29%|██▊       | 34/119 [00:14<00:24,  3.50it/s]\n",
      "Inference:  29%|██▉       | 35/119 [00:14<00:21,  3.92it/s]\n",
      "Inference:  30%|███       | 36/119 [00:15<00:34,  2.39it/s]\n",
      "Inference:  31%|███       | 37/119 [00:15<00:27,  2.94it/s]\n",
      "Inference:  32%|███▏      | 38/119 [00:15<00:23,  3.48it/s]\n",
      "Inference:  33%|███▎      | 39/119 [00:16<00:20,  3.99it/s]\n",
      "Inference:  34%|███▎      | 40/119 [00:16<00:27,  2.89it/s]\n",
      "Inference:  34%|███▍      | 41/119 [00:16<00:22,  3.44it/s]\n",
      "Inference:  35%|███▌      | 42/119 [00:16<00:19,  3.97it/s]\n",
      "Inference:  36%|███▌      | 43/119 [00:17<00:24,  3.07it/s]\n",
      "Inference:  37%|███▋      | 44/119 [00:17<00:20,  3.62it/s]\n",
      "Inference:  38%|███▊      | 45/119 [00:17<00:18,  4.11it/s]\n",
      "Inference:  39%|███▊      | 46/119 [00:18<00:19,  3.75it/s]\n",
      "Inference:  39%|███▉      | 47/119 [00:18<00:29,  2.42it/s]\n",
      "Inference:  40%|████      | 48/119 [00:18<00:23,  2.97it/s]\n",
      "Inference:  41%|████      | 49/119 [00:19<00:19,  3.53it/s]\n",
      "Inference:  42%|████▏     | 50/119 [00:19<00:17,  4.05it/s]\n",
      "Inference:  43%|████▎     | 51/119 [00:20<00:30,  2.25it/s]\n",
      "Inference:  44%|████▎     | 52/119 [00:20<00:24,  2.79it/s]\n",
      "Inference:  45%|████▍     | 53/119 [00:20<00:19,  3.33it/s]\n",
      "Inference:  45%|████▌     | 54/119 [00:20<00:16,  3.86it/s]\n",
      "Inference:  46%|████▌     | 55/119 [00:21<00:30,  2.08it/s]\n",
      "Inference:  47%|████▋     | 56/119 [00:21<00:24,  2.61it/s]\n",
      "Inference:  48%|████▊     | 57/119 [00:21<00:19,  3.17it/s]\n",
      "Inference:  49%|████▊     | 58/119 [00:22<00:16,  3.68it/s]\n",
      "Inference:  50%|████▉     | 59/119 [00:22<00:20,  2.96it/s]\n",
      "Inference:  50%|█████     | 60/119 [00:22<00:16,  3.51it/s]\n",
      "Inference:  51%|█████▏    | 61/119 [00:22<00:14,  4.05it/s]\n",
      "Inference:  52%|█████▏    | 62/119 [00:23<00:19,  2.86it/s]\n",
      "Inference:  53%|█████▎    | 63/119 [00:23<00:19,  2.88it/s]\n",
      "Inference:  54%|█████▍    | 64/119 [00:24<00:19,  2.82it/s]\n",
      "Inference:  55%|█████▍    | 65/119 [00:24<00:16,  3.37it/s]\n",
      "Inference:  55%|█████▌    | 66/119 [00:24<00:20,  2.64it/s]\n",
      "Inference:  56%|█████▋    | 67/119 [00:25<00:18,  2.82it/s]\n",
      "Inference:  57%|█████▋    | 68/119 [00:26<00:23,  2.16it/s]\n",
      "Inference:  58%|█████▊    | 69/119 [00:26<00:18,  2.69it/s]\n",
      "Inference:  59%|█████▉    | 70/119 [00:26<00:15,  3.23it/s]\n",
      "Inference:  60%|█████▉    | 71/119 [00:26<00:13,  3.58it/s]\n",
      "Inference:  61%|██████    | 72/119 [00:27<00:29,  1.61it/s]\n",
      "Inference:  61%|██████▏   | 73/119 [00:28<00:22,  2.08it/s]\n",
      "Inference:  62%|██████▏   | 74/119 [00:28<00:17,  2.60it/s]\n",
      "Inference:  63%|██████▎   | 75/119 [00:28<00:13,  3.15it/s]\n",
      "Inference:  64%|██████▍   | 76/119 [00:29<00:25,  1.68it/s]\n",
      "Inference:  65%|██████▍   | 77/119 [00:29<00:19,  2.15it/s]\n",
      "Inference:  66%|██████▌   | 78/119 [00:29<00:15,  2.68it/s]\n",
      "Inference:  66%|██████▋   | 79/119 [00:30<00:12,  3.23it/s]\n",
      "Inference:  67%|██████▋   | 80/119 [00:31<00:20,  1.92it/s]\n",
      "Inference:  68%|██████▊   | 81/119 [00:31<00:15,  2.43it/s]\n",
      "Inference:  69%|██████▉   | 82/119 [00:31<00:12,  2.97it/s]\n",
      "Inference:  70%|██████▉   | 83/119 [00:31<00:10,  3.53it/s]\n",
      "Inference:  71%|███████   | 84/119 [00:32<00:17,  1.96it/s]\n",
      "Inference:  71%|███████▏  | 85/119 [00:32<00:13,  2.47it/s]\n",
      "Inference:  72%|███████▏  | 86/119 [00:33<00:10,  3.02it/s]\n",
      "Inference:  73%|███████▎  | 87/119 [00:33<00:08,  3.57it/s]\n",
      "Inference:  74%|███████▍  | 88/119 [00:34<00:16,  1.94it/s]\n",
      "Inference:  75%|███████▍  | 89/119 [00:34<00:12,  2.44it/s]\n",
      "Inference:  76%|███████▌  | 90/119 [00:34<00:09,  2.99it/s]\n",
      "Inference:  76%|███████▋  | 91/119 [00:34<00:07,  3.54it/s]\n",
      "Inference:  77%|███████▋  | 92/119 [00:35<00:14,  1.83it/s]\n",
      "Inference:  78%|███████▊  | 93/119 [00:36<00:11,  2.33it/s]\n",
      "Inference:  79%|███████▉  | 94/119 [00:36<00:08,  2.87it/s]\n",
      "Inference:  80%|███████▉  | 95/119 [00:36<00:07,  3.42it/s]\n",
      "Inference:  81%|████████  | 96/119 [00:37<00:09,  2.46it/s]\n",
      "Inference:  82%|████████▏ | 97/119 [00:37<00:07,  3.01it/s]\n",
      "Inference:  82%|████████▏ | 98/119 [00:37<00:05,  3.58it/s]\n",
      "Inference:  83%|████████▎ | 99/119 [00:37<00:04,  4.09it/s]\n",
      "Inference:  84%|████████▍ | 100/119 [00:38<00:08,  2.25it/s]\n",
      "Inference:  85%|████████▍ | 101/119 [00:38<00:06,  2.58it/s]\n",
      "Inference:  86%|████████▌ | 102/119 [00:38<00:05,  3.13it/s]\n",
      "Inference:  87%|████████▋ | 103/119 [00:38<00:04,  3.69it/s]\n",
      "Inference:  87%|████████▋ | 104/119 [00:39<00:05,  2.60it/s]\n",
      "Inference:  88%|████████▊ | 105/119 [00:39<00:04,  2.92it/s]\n",
      "Inference:  89%|████████▉ | 106/119 [00:40<00:03,  3.47it/s]\n",
      "Inference:  90%|████████▉ | 107/119 [00:40<00:05,  2.14it/s]\n",
      "Inference:  91%|█████████ | 108/119 [00:41<00:04,  2.65it/s]\n",
      "Inference:  92%|█████████▏| 109/119 [00:41<00:03,  3.19it/s]\n",
      "Inference:  92%|█████████▏| 110/119 [00:41<00:02,  3.74it/s]\n",
      "Inference:  93%|█████████▎| 111/119 [00:42<00:03,  2.14it/s]\n",
      "Inference:  94%|█████████▍| 112/119 [00:43<00:03,  1.93it/s]\n",
      "Inference:  95%|█████████▍| 113/119 [00:43<00:02,  2.43it/s]\n",
      "Inference:  96%|█████████▌| 114/119 [00:43<00:01,  2.98it/s]\n",
      "Inference:  97%|█████████▋| 115/119 [00:43<00:01,  2.66it/s]\n",
      "Inference:  97%|█████████▋| 116/119 [00:44<00:01,  2.33it/s]\n",
      "Inference:  98%|█████████▊| 117/119 [00:44<00:00,  2.88it/s]\n",
      "Inference:  99%|█████████▉| 118/119 [00:44<00:00,  3.44it/s]\n",
      "Inference: 100%|██████████| 119/119 [00:45<00:00,  1.69it/s]\n",
      "Inference: 100%|██████████| 119/119 [00:46<00:00,  2.59it/s]\n",
      "Results exported to: outputs/ClassifierSegExperiment_224_epo30_bs32_lr0.0001_s42/2023-12-19_2344_VGG16_BN_Attention/ClassifierSegExperiment_224_epo30_bs32_lr0.0001_s42_2023-12-19_2344_VGG16_BN_Attention.csv\n",
      "Majority vote AUC: 0.87659\n",
      "Majority vote accuracy: 0.8769757639620653\n",
      "Majority vote kappa: 0.7537033445278865\n",
      "Target 0 - Precision: 0.86454, Recall: 0.89902, F-score: 0.88144, Sensitivity: 0.8990160538581046, Specificity: 0.8541554959785522, Support: 1931\n",
      "Target 1 - Precision: 0.89094, Recall: 0.85416, F-score: 0.87216, Sensitivity: 0.8541554959785522, Specificity: 0.8990160538581046, Support: 1865\n",
      "Figure(1500x600)\n",
      "(   {   'accuracy': 0.8769757639620653,\n",
      "        'auc': 0.8765857749183283,\n",
      "        'fscore': [0.8814419903528814, 0.8721598686011496],\n",
      "        'kappa': 0.7537033445278865,\n",
      "        'precision': [0.8645418326693227, 0.8909395973154363],\n",
      "        'recall': [0.8990160538581046, 0.8541554959785522],\n",
      "        'sensitivity': [0.8990160538581046, 0.8541554959785522],\n",
      "        'specificity': [0.8541554959785522, 0.8990160538581046],\n",
      "        'support': [1931, 1865]},)\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../inference.py --test_path \"../datasets/challenge1/val\" \\\n",
    "                    --experiment_name \"ClassifierSegExperiment\" \\\n",
    "                    --network_name \"VGG16_BN_Attention\" \\\n",
    "                    --max_epochs \"30\" \\\n",
    "                    --batch_size \"32\" \\\n",
    "                    --timeframe \"2023-12-19_2344\" \\\n",
    "                    --verbose \"2\" \\\n",
    "                    --report'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the center crop `A.CenterCrop`reduces the result significantlly! Was trained on few epochs and ended the training as it wasn't learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results didn't improve with masks. Giving another try to build an ensemble and train it with the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting training pipeline with 30 epochs and 5 folds.\n",
      "Loading data with 2 class labels...\n",
      "Loading the data from ../datasets/challenge1/train\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Using segmentation masks for training...\n",
      "train_images: 15195, train_masks: 15195, train_labels: 15195\n",
      "val_images: 3796, val_masks: 3796, val_labels: 3796\n",
      "Class weights: [0.98342828 1.0171398 ]\n",
      "Creating a classifier experiment using VGG16_BN_Attention network.\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using cross entropy loss.\n",
      "Using dice loss.\n",
      "[CV 1/5]: Epoch: 1 Train batch 10 loss: 0.6233866214752197, 2.1% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 20 loss: 0.5773177742958069, 4.2% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 30 loss: 0.5194941163063049, 6.3% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 40 loss: 0.4522211253643036, 8.4% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 50 loss: 0.45502814650535583, 10.5% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 60 loss: 0.37401607632637024, 12.6% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 70 loss: 0.7588430047035217, 14.7% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 80 loss: 0.34380653500556946, 16.8% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 90 loss: 0.7001309990882874, 18.9% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 100 loss: 0.47315603494644165, 21.1% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 110 loss: 0.34326478838920593, 23.2% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 120 loss: 0.6348642706871033, 25.3% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 130 loss: 0.3867015540599823, 27.4% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 140 loss: 0.5341516733169556, 29.5% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 150 loss: 0.589958667755127, 31.6% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 160 loss: 0.3556729257106781, 33.7% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 170 loss: 0.42746809124946594, 35.8% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 180 loss: 0.4764305353164673, 37.9% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 190 loss: 0.46680948138237, 40.0% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 200 loss: 0.35304638743400574, 42.1% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 210 loss: 0.717607855796814, 44.2% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 220 loss: 0.5108252167701721, 46.3% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 230 loss: 0.40862706303596497, 48.4% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 240 loss: 0.29750770330429077, 50.5% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 250 loss: 0.4291023313999176, 52.6% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 260 loss: 0.47933006286621094, 54.7% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 270 loss: 0.5628179907798767, 56.8% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 280 loss: 0.4135713577270508, 58.9% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 290 loss: 0.4063308537006378, 61.1% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 300 loss: 0.4027697741985321, 63.2% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 310 loss: 0.36650219559669495, 65.3% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 320 loss: 0.5831911563873291, 67.4% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 330 loss: 0.4795185923576355, 69.5% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 340 loss: 0.3951939642429352, 71.6% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 350 loss: 0.4342324137687683, 73.7% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 360 loss: 0.3543739318847656, 75.8% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 370 loss: 0.3536383807659149, 77.9% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 380 loss: 0.34510520100593567, 80.0% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 390 loss: 0.4754268229007721, 82.1% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 400 loss: 0.3485831618309021, 84.2% complete\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "[CV 1/5]: Epoch: 1 Train batch 410 loss: 0.3585801422595978, 86.3% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 420 loss: 0.34591856598854065, 88.4% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 430 loss: 0.361551970243454, 90.5% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 440 loss: 0.4549682140350342, 92.6% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 450 loss: 0.32635200023651123, 94.7% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 460 loss: 0.3165263235569, 96.8% complete\n",
      "[CV 1/5]: Epoch: 1 Train batch 470 loss: 0.30603715777397156, 98.9% complete\n",
      "[CV 1/5]: Epoch: 1, valid batch 1, loss 0.5164188742637634\n",
      "[CV 1/5]: Epoch: 1, valid batch 2, loss 0.2519562840461731\n",
      "[CV 1/5]: Epoch: 1, valid batch 3, loss 0.42436978220939636\n",
      "[CV 1/5]: Epoch: 1, valid batch 4, loss 0.3397139608860016\n",
      "[CV 1/5]: Epoch: 1, valid batch 5, loss 0.34300071001052856\n",
      "[CV 1/5]: Epoch: 1, valid batch 6, loss 0.2606601417064667\n",
      "[CV 1/5]: Epoch: 1, valid batch 7, loss 0.3407219648361206\n",
      "[CV 1/5]: Epoch: 1, valid batch 8, loss 0.3798618018627167\n",
      "[CV 1/5]: Epoch: 1, valid batch 9, loss 0.61174076795578\n",
      "[CV 1/5]: Epoch: 1, valid batch 10, loss 0.37668928503990173\n",
      "[CV 1/5]: Epoch: 1, valid batch 11, loss 0.3859517276287079\n",
      "[CV 1/5]: Epoch: 1, valid batch 12, loss 0.3581363260746002\n",
      "[CV 1/5]: Epoch: 1, valid batch 13, loss 0.16288286447525024\n",
      "[CV 1/5]: Epoch: 1, valid batch 14, loss 0.21362286806106567\n",
      "[CV 1/5]: Epoch: 1, valid batch 15, loss 0.3037617802619934\n",
      "[CV 1/5]: Epoch: 1, valid batch 16, loss 0.3448909819126129\n",
      "[CV 1/5]: Epoch: 1, valid batch 17, loss 0.4190988540649414\n",
      "[CV 1/5]: Epoch: 1, valid batch 18, loss 0.2962397336959839\n",
      "[CV 1/5]: Epoch: 1, valid batch 19, loss 0.4484999179840088\n",
      "[CV 1/5]: Epoch: 1, valid batch 20, loss 0.3578959107398987\n",
      "[CV 1/5]: Epoch: 1, valid batch 21, loss 0.228044793009758\n",
      "[CV 1/5]: Epoch: 1, valid batch 22, loss 0.46294981241226196\n",
      "[CV 1/5]: Epoch: 1, valid batch 23, loss 0.32132601737976074\n",
      "[CV 1/5]: Epoch: 1, valid batch 24, loss 0.337024450302124\n",
      "[CV 1/5]: Epoch: 1, valid batch 25, loss 0.30654558539390564\n",
      "[CV 1/5]: Epoch: 1, valid batch 26, loss 0.27783697843551636\n",
      "[CV 1/5]: Epoch: 1, valid batch 27, loss 0.4081069231033325\n",
      "[CV 1/5]: Epoch: 1, valid batch 28, loss 0.36315688490867615\n",
      "[CV 1/5]: Epoch: 1, valid batch 29, loss 0.3508793115615845\n",
      "[CV 1/5]: Epoch: 1, valid batch 30, loss 0.4573746919631958\n",
      "[CV 1/5]: Epoch: 1, valid batch 31, loss 0.3154209852218628\n",
      "[CV 1/5]: Epoch: 1, valid batch 32, loss 0.33014988899230957\n",
      "[CV 1/5]: Epoch: 1, valid batch 33, loss 0.2082129716873169\n",
      "[CV 1/5]: Epoch: 1, valid batch 34, loss 0.2988590598106384\n",
      "[CV 1/5]: Epoch: 1, valid batch 35, loss 0.3946959376335144\n",
      "[CV 1/5]: Epoch: 1, valid batch 36, loss 0.25413841009140015\n",
      "[CV 1/5]: Epoch: 1, valid batch 37, loss 0.3904951810836792\n",
      "[CV 1/5]: Epoch: 1, valid batch 38, loss 0.3721221685409546\n",
      "[CV 1/5]: Epoch: 1, valid batch 39, loss 0.5047473311424255\n",
      "[CV 1/5]: Epoch: 1, valid batch 40, loss 0.25319328904151917\n",
      "[CV 1/5]: Epoch: 1, valid batch 41, loss 0.20403151214122772\n",
      "[CV 1/5]: Epoch: 1, valid batch 42, loss 0.3445815145969391\n",
      "[CV 1/5]: Epoch: 1, valid batch 43, loss 0.4173014760017395\n",
      "[CV 1/5]: Epoch: 1, valid batch 44, loss 0.24968016147613525\n",
      "[CV 1/5]: Epoch: 1, valid batch 45, loss 0.34345781803131104\n",
      "[CV 1/5]: Epoch: 1, valid batch 46, loss 0.3106748163700104\n",
      "[CV 1/5]: Epoch: 1, valid batch 47, loss 0.47006893157958984\n",
      "[CV 1/5]: Epoch: 1, valid batch 48, loss 0.24574671685695648\n",
      "[CV 1/5]: Epoch: 1, valid batch 49, loss 0.2794796824455261\n",
      "[CV 1/5]: Epoch: 1, valid batch 50, loss 0.2717498242855072\n",
      "[CV 1/5]: Epoch: 1, valid batch 51, loss 0.39731359481811523\n",
      "[CV 1/5]: Epoch: 1, valid batch 52, loss 0.20912790298461914\n",
      "[CV 1/5]: Epoch: 1, valid batch 53, loss 0.29415494203567505\n",
      "[CV 1/5]: Epoch: 1, valid batch 54, loss 0.3356906771659851\n",
      "[CV 1/5]: Epoch: 1, valid batch 55, loss 0.7076463103294373\n",
      "[CV 1/5]: Epoch: 1, valid batch 56, loss 0.3683258891105652\n",
      "[CV 1/5]: Epoch: 1, valid batch 57, loss 0.4507993161678314\n",
      "[CV 1/5]: Epoch: 1, valid batch 58, loss 0.2992956042289734\n",
      "[CV 1/5]: Epoch: 1, valid batch 59, loss 0.3786192238330841\n",
      "[CV 1/5]: Epoch: 1, valid batch 60, loss 0.4480167031288147\n",
      "[CV 1/5]: Epoch: 1, valid batch 61, loss 0.47438791394233704\n",
      "[CV 1/5]: Epoch: 1, valid batch 62, loss 0.4809090495109558\n",
      "[CV 1/5]: Epoch: 1, valid batch 63, loss 0.37000811100006104\n",
      "[CV 1/5]: Epoch: 1, valid batch 64, loss 0.4487288296222687\n",
      "[CV 1/5]: Epoch: 1, valid batch 65, loss 0.3060752749443054\n",
      "[CV 1/5]: Epoch: 1, valid batch 66, loss 0.8668594360351562\n",
      "[CV 1/5]: Epoch: 1, valid batch 67, loss 0.3324137032032013\n",
      "[CV 1/5]: Epoch: 1, valid batch 68, loss 0.6998996734619141\n",
      "[CV 1/5]: Epoch: 1, valid batch 69, loss 0.38109448552131653\n",
      "[CV 1/5]: Epoch: 1, valid batch 70, loss 0.3086634874343872\n",
      "[CV 1/5]: Epoch: 1, valid batch 71, loss 0.4169713854789734\n",
      "[CV 1/5]: Epoch: 1, valid batch 72, loss 0.23125407099723816\n",
      "[CV 1/5]: Epoch: 1, valid batch 73, loss 0.21104395389556885\n",
      "[CV 1/5]: Epoch: 1, valid batch 74, loss 0.5695614814758301\n",
      "[CV 1/5]: Epoch: 1, valid batch 75, loss 0.3552596867084503\n",
      "[CV 1/5]: Epoch: 1, valid batch 76, loss 0.48333877325057983\n",
      "[CV 1/5]: Epoch: 1, valid batch 77, loss 0.29858845472335815\n",
      "[CV 1/5]: Epoch: 1, valid batch 78, loss 0.36331555247306824\n",
      "[CV 1/5]: Epoch: 1, valid batch 79, loss 0.5967297554016113\n",
      "[CV 1/5]: Epoch: 1, valid batch 80, loss 0.4447649121284485\n",
      "[CV 1/5]: Epoch: 1, valid batch 81, loss 0.32008832693099976\n",
      "[CV 1/5]: Epoch: 1, valid batch 82, loss 0.45835915207862854\n",
      "[CV 1/5]: Epoch: 1, valid batch 83, loss 0.5633860230445862\n",
      "[CV 1/5]: Epoch: 1, valid batch 84, loss 0.35501593351364136\n",
      "[CV 1/5]: Epoch: 1, valid batch 85, loss 0.44295310974121094\n",
      "[CV 1/5]: Epoch: 1, valid batch 86, loss 0.3053053319454193\n",
      "[CV 1/5]: Epoch: 1, valid batch 87, loss 0.4941102862358093\n",
      "[CV 1/5]: Epoch: 1, valid batch 88, loss 0.4241201877593994\n",
      "[CV 1/5]: Epoch: 1, valid batch 89, loss 0.2831677496433258\n",
      "[CV 1/5]: Epoch: 1, valid batch 90, loss 0.3854829668998718\n",
      "[CV 1/5]: Epoch: 1, valid batch 91, loss 0.48273372650146484\n",
      "[CV 1/5]: Epoch: 1, valid batch 92, loss 0.2697790563106537\n",
      "[CV 1/5]: Epoch: 1, valid batch 93, loss 0.17503051459789276\n",
      "[CV 1/5]: Epoch: 1, valid batch 94, loss 0.3180027902126312\n",
      "[CV 1/5]: Epoch: 1, valid batch 95, loss 0.7171337604522705\n",
      "[CV 1/5]: Epoch: 1, valid batch 96, loss 0.30484405159950256\n",
      "[CV 1/5]: Epoch: 1, valid batch 97, loss 0.4389336109161377\n",
      "[CV 1/5]: Epoch: 1, valid batch 98, loss 0.4225479066371918\n",
      "[CV 1/5]: Epoch: 1, valid batch 99, loss 0.32967323064804077\n",
      "[CV 1/5]: Epoch: 1, valid batch 100, loss 0.5855495929718018\n",
      "[CV 1/5]: Epoch: 1, valid batch 101, loss 0.6084568500518799\n",
      "[CV 1/5]: Epoch: 1, valid batch 102, loss 0.49139270186424255\n",
      "[CV 1/5]: Epoch: 1, valid batch 103, loss 0.2682884633541107\n",
      "[CV 1/5]: Epoch: 1, valid batch 104, loss 0.31669262051582336\n",
      "[CV 1/5]: Epoch: 1, valid batch 105, loss 0.3319959044456482\n",
      "[CV 1/5]: Epoch: 1, valid batch 106, loss 0.29086071252822876\n",
      "[CV 1/5]: Epoch: 1, valid batch 107, loss 0.5768511295318604\n",
      "[CV 1/5]: Epoch: 1, valid batch 108, loss 0.3798754811286926\n",
      "[CV 1/5]: Epoch: 1, valid batch 109, loss 0.5109410285949707\n",
      "[CV 1/5]: Epoch: 1, valid batch 110, loss 0.3051771819591522\n",
      "[CV 1/5]: Epoch: 1, valid batch 111, loss 0.38156652450561523\n",
      "[CV 1/5]: Epoch: 1, valid batch 112, loss 0.4316715896129608\n",
      "[CV 1/5]: Epoch: 1, valid batch 113, loss 0.2931853234767914\n",
      "[CV 1/5]: Epoch: 1, valid batch 114, loss 0.5589664578437805\n",
      "[CV 1/5]: Epoch: 1, valid batch 115, loss 0.4019067883491516\n",
      "[CV 1/5]: Epoch: 1, valid batch 116, loss 0.2705933451652527\n",
      "[CV 1/5]: Epoch: 1, valid batch 117, loss 0.4159872829914093\n",
      "[CV 1/5]: Epoch: 1, valid batch 118, loss 0.3793645203113556\n",
      "[CV 1/5]: Epoch: 1, valid batch 119, loss 0.3988794684410095\n",
      "Valid loss improved from inf to 0.380890. Saving model ...\n",
      "[CV 1/5]: Epoch: 01/30 | epoch time: 32.0m 42.21s | lr: 1.00000e-04 | train/loss: 0.42801 | val/loss: 0.38089 | val/accuracy: 0.83601 | val/AUC: 0.83534 | val/Kappa: 0.67151\n",
      "[CV 1/5]: Epoch: 2 Train batch 10 loss: 0.3065042793750763, 2.1% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 20 loss: 0.47732746601104736, 4.2% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 30 loss: 0.5432558655738831, 6.3% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 40 loss: 0.2701385021209717, 8.4% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 50 loss: 0.5226427316665649, 10.5% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 60 loss: 0.2857414186000824, 12.6% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 70 loss: 0.3185461461544037, 14.7% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 80 loss: 0.2290324717760086, 16.8% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 90 loss: 0.31805679202079773, 18.9% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 100 loss: 0.5060223937034607, 21.1% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 110 loss: 0.2902167737483978, 23.2% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 120 loss: 0.5239502191543579, 25.3% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 130 loss: 0.5110781788825989, 27.4% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 140 loss: 0.4305226802825928, 29.5% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 150 loss: 0.5075034499168396, 31.6% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 160 loss: 0.3672526180744171, 33.7% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 170 loss: 0.29440295696258545, 35.8% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 180 loss: 0.3919644355773926, 37.9% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 190 loss: 0.4156270921230316, 40.0% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 200 loss: 0.3354247212409973, 42.1% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 210 loss: 0.489653617143631, 44.2% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 220 loss: 0.3137955665588379, 46.3% complete\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([24, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([23, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "[CV 1/5]: Epoch: 2 Train batch 230 loss: 0.38372892141342163, 48.4% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 240 loss: 0.3109035789966583, 50.5% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 250 loss: 0.3623752295970917, 52.6% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 260 loss: 0.2974776327610016, 54.7% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 270 loss: 0.3615075647830963, 56.8% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 280 loss: 0.3542746603488922, 58.9% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 290 loss: 0.18513599038124084, 61.1% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 300 loss: 0.3070203363895416, 63.2% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 310 loss: 0.631428062915802, 65.3% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 320 loss: 0.41030773520469666, 67.4% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 330 loss: 0.23264183104038239, 69.5% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 340 loss: 0.41063353419303894, 71.6% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 350 loss: 0.4212373197078705, 73.7% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 360 loss: 0.4068163335323334, 75.8% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 370 loss: 0.34565168619155884, 77.9% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 380 loss: 0.28920015692710876, 80.0% complete\n",
      "[CV 1/5]: Epoch: 2 Train batch 390 loss: 0.2814434766769409, 82.1% complete\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../train_cv.py --train_path \"../datasets/challenge1/train\" \\\n",
    "                        --train_masks_path \"../datasets_masks/challenge1/train\" \\\n",
    "                        --valid_masks_path \"../datasets_masks/challenge1/val\"\\\n",
    "                        --valid_path \"../datasets/challenge1/val\" \\\n",
    "                        --experiment_name \"ClassifierSegExperimentCV\" \\\n",
    "                        --network_name \"VGG16_BN_Attention\" \\\n",
    "                        --max_epochs \"30\" \\\n",
    "                        --num_folds \"5\" \\\n",
    "                        --batch_size \"32\" \\\n",
    "                        --verbose \"2\"'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the above ensemble finished on the server. Evaluating below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting inference script...\n",
      "Constructed output path: outputs/ClassifierSegExperimentCV_224_epo30_bs32_lr0.0001_s42/2023-12-20_1507_VGG16_BN_Attention. Searching for models...\n",
      "Found 5 models. Starting loading the models.\n",
      "Loading data with 2 class labels from ../datasets/challenge1/val path...\n",
      "Dataset labels: {'nevus': 0, 'others': 1} dictionary.\n",
      "Loading the data from ../datasets/challenge1/val\n",
      "Dataset length: 3796\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Using VGG16_BN_Attention with configurations: num_classes='2', normalize_attn='False'\n",
      "Namespace(test_path='../datasets/challenge1/val', output='outputs', experiment_name='ClassifierSegExperimentCV', network_name='VGG16_BN_Attention', max_epochs=30, batch_size=32, base_lr=0.0001, img_size=224, seed=42, timeframe='2023-12-20_1507', verbose=2, multi=False, report=True, ensemble=True)\n",
      "\n",
      "Inference:   0%|          | 0/119 [00:00<?, ?it/s]\n",
      "Inference:   1%|          | 1/119 [00:05<11:45,  5.97s/it]\n",
      "Inference:   2%|▏         | 2/119 [00:06<05:39,  2.90s/it]\n",
      "Inference:   3%|▎         | 3/119 [00:07<03:42,  1.91s/it]\n",
      "Inference:   3%|▎         | 4/119 [00:08<02:47,  1.46s/it]\n",
      "Inference:   4%|▍         | 5/119 [00:08<02:17,  1.20s/it]\n",
      "Inference:   5%|▌         | 6/119 [00:09<01:58,  1.05s/it]\n",
      "Inference:   6%|▌         | 7/119 [00:10<01:46,  1.05it/s]\n",
      "Inference:   7%|▋         | 8/119 [00:11<01:38,  1.12it/s]\n",
      "Inference:   8%|▊         | 9/119 [00:11<01:32,  1.18it/s]\n",
      "Inference:   8%|▊         | 10/119 [00:12<01:28,  1.23it/s]\n",
      "Inference:   9%|▉         | 11/119 [00:13<01:25,  1.26it/s]\n",
      "Inference:  10%|█         | 12/119 [00:14<01:23,  1.28it/s]\n",
      "Inference:  11%|█         | 13/119 [00:14<01:21,  1.30it/s]\n",
      "Inference:  12%|█▏        | 14/119 [00:15<01:20,  1.31it/s]\n",
      "Inference:  13%|█▎        | 15/119 [00:16<01:18,  1.32it/s]\n",
      "Inference:  13%|█▎        | 16/119 [00:17<01:17,  1.32it/s]\n",
      "Inference:  14%|█▍        | 17/119 [00:17<01:17,  1.32it/s]\n",
      "Inference:  15%|█▌        | 18/119 [00:18<01:16,  1.32it/s]\n",
      "Inference:  16%|█▌        | 19/119 [00:19<01:15,  1.33it/s]\n",
      "Inference:  17%|█▋        | 20/119 [00:20<01:14,  1.33it/s]\n",
      "Inference:  18%|█▊        | 21/119 [00:20<01:13,  1.33it/s]\n",
      "Inference:  18%|█▊        | 22/119 [00:21<01:12,  1.33it/s]\n",
      "Inference:  19%|█▉        | 23/119 [00:22<01:11,  1.33it/s]\n",
      "Inference:  20%|██        | 24/119 [00:23<01:11,  1.33it/s]\n",
      "Inference:  21%|██        | 25/119 [00:23<01:10,  1.33it/s]\n",
      "Inference:  22%|██▏       | 26/119 [00:24<01:09,  1.33it/s]\n",
      "Inference:  23%|██▎       | 27/119 [00:25<01:08,  1.33it/s]\n",
      "Inference:  24%|██▎       | 28/119 [00:26<01:08,  1.33it/s]\n",
      "Inference:  24%|██▍       | 29/119 [00:26<01:07,  1.33it/s]\n",
      "Inference:  25%|██▌       | 30/119 [00:27<01:06,  1.33it/s]\n",
      "Inference:  26%|██▌       | 31/119 [00:28<01:05,  1.34it/s]\n",
      "Inference:  27%|██▋       | 32/119 [00:29<01:05,  1.34it/s]\n",
      "Inference:  28%|██▊       | 33/119 [00:29<01:04,  1.34it/s]\n",
      "Inference:  29%|██▊       | 34/119 [00:30<01:03,  1.34it/s]\n",
      "Inference:  29%|██▉       | 35/119 [00:31<01:02,  1.34it/s]\n",
      "Inference:  30%|███       | 36/119 [00:32<01:02,  1.33it/s]\n",
      "Inference:  31%|███       | 37/119 [00:32<01:01,  1.33it/s]\n",
      "Inference:  32%|███▏      | 38/119 [00:33<01:00,  1.33it/s]\n",
      "Inference:  33%|███▎      | 39/119 [00:34<01:00,  1.32it/s]\n",
      "Inference:  34%|███▎      | 40/119 [00:35<00:59,  1.32it/s]\n",
      "Inference:  34%|███▍      | 41/119 [00:35<00:58,  1.33it/s]\n",
      "Inference:  35%|███▌      | 42/119 [00:36<00:57,  1.33it/s]\n",
      "Inference:  36%|███▌      | 43/119 [00:37<00:57,  1.33it/s]\n",
      "Inference:  37%|███▋      | 44/119 [00:38<00:56,  1.33it/s]\n",
      "Inference:  38%|███▊      | 45/119 [00:38<00:55,  1.33it/s]\n",
      "Inference:  39%|███▊      | 46/119 [00:39<00:54,  1.34it/s]\n",
      "Inference:  39%|███▉      | 47/119 [00:40<00:53,  1.34it/s]\n",
      "Inference:  40%|████      | 48/119 [00:41<00:53,  1.34it/s]\n",
      "Inference:  41%|████      | 49/119 [00:41<00:52,  1.34it/s]\n",
      "Inference:  42%|████▏     | 50/119 [00:42<00:51,  1.34it/s]\n",
      "Inference:  43%|████▎     | 51/119 [00:43<00:50,  1.34it/s]\n",
      "Inference:  44%|████▎     | 52/119 [00:44<00:50,  1.34it/s]\n",
      "Inference:  45%|████▍     | 53/119 [00:44<00:49,  1.34it/s]\n",
      "Inference:  45%|████▌     | 54/119 [00:45<00:48,  1.34it/s]\n",
      "Inference:  46%|████▌     | 55/119 [00:46<00:47,  1.34it/s]\n",
      "Inference:  47%|████▋     | 56/119 [00:47<00:47,  1.33it/s]\n",
      "Inference:  48%|████▊     | 57/119 [00:47<00:46,  1.34it/s]\n",
      "Inference:  49%|████▊     | 58/119 [00:48<00:45,  1.34it/s]\n",
      "Inference:  50%|████▉     | 59/119 [00:49<00:44,  1.34it/s]\n",
      "Inference:  50%|█████     | 60/119 [00:50<00:44,  1.34it/s]\n",
      "Inference:  51%|█████▏    | 61/119 [00:50<00:43,  1.34it/s]\n",
      "Inference:  52%|█████▏    | 62/119 [00:51<00:42,  1.34it/s]\n",
      "Inference:  53%|█████▎    | 63/119 [00:52<00:41,  1.34it/s]\n",
      "Inference:  54%|█████▍    | 64/119 [00:53<00:41,  1.34it/s]\n",
      "Inference:  55%|█████▍    | 65/119 [00:53<00:40,  1.34it/s]\n",
      "Inference:  55%|█████▌    | 66/119 [00:54<00:39,  1.34it/s]\n",
      "Inference:  56%|█████▋    | 67/119 [00:55<00:38,  1.34it/s]\n",
      "Inference:  57%|█████▋    | 68/119 [00:56<00:38,  1.34it/s]\n",
      "Inference:  58%|█████▊    | 69/119 [00:56<00:37,  1.34it/s]\n",
      "Inference:  59%|█████▉    | 70/119 [00:57<00:36,  1.34it/s]\n",
      "Inference:  60%|█████▉    | 71/119 [00:58<00:35,  1.34it/s]\n",
      "Inference:  61%|██████    | 72/119 [00:59<00:35,  1.34it/s]\n",
      "Inference:  61%|██████▏   | 73/119 [00:59<00:34,  1.34it/s]\n",
      "Inference:  62%|██████▏   | 74/119 [01:00<00:33,  1.33it/s]\n",
      "Inference:  63%|██████▎   | 75/119 [01:01<00:32,  1.33it/s]\n",
      "Inference:  64%|██████▍   | 76/119 [01:02<00:32,  1.33it/s]\n",
      "Inference:  65%|██████▍   | 77/119 [01:02<00:31,  1.34it/s]\n",
      "Inference:  66%|██████▌   | 78/119 [01:03<00:30,  1.34it/s]\n",
      "Inference:  66%|██████▋   | 79/119 [01:04<00:29,  1.34it/s]\n",
      "Inference:  67%|██████▋   | 80/119 [01:05<00:29,  1.33it/s]\n",
      "Inference:  68%|██████▊   | 81/119 [01:05<00:28,  1.33it/s]\n",
      "Inference:  69%|██████▉   | 82/119 [01:06<00:27,  1.34it/s]\n",
      "Inference:  70%|██████▉   | 83/119 [01:07<00:26,  1.34it/s]\n",
      "Inference:  71%|███████   | 84/119 [01:08<00:26,  1.34it/s]\n",
      "Inference:  71%|███████▏  | 85/119 [01:08<00:25,  1.34it/s]\n",
      "Inference:  72%|███████▏  | 86/119 [01:09<00:24,  1.34it/s]\n",
      "Inference:  73%|███████▎  | 87/119 [01:10<00:23,  1.34it/s]\n",
      "Inference:  74%|███████▍  | 88/119 [01:11<00:23,  1.34it/s]\n",
      "Inference:  75%|███████▍  | 89/119 [01:11<00:22,  1.34it/s]\n",
      "Inference:  76%|███████▌  | 90/119 [01:12<00:21,  1.34it/s]\n",
      "Inference:  76%|███████▋  | 91/119 [01:13<00:20,  1.34it/s]\n",
      "Inference:  77%|███████▋  | 92/119 [01:14<00:20,  1.34it/s]\n",
      "Inference:  78%|███████▊  | 93/119 [01:14<00:19,  1.34it/s]\n",
      "Inference:  79%|███████▉  | 94/119 [01:15<00:18,  1.34it/s]\n",
      "Inference:  80%|███████▉  | 95/119 [01:16<00:17,  1.34it/s]\n",
      "Inference:  81%|████████  | 96/119 [01:17<00:17,  1.33it/s]\n",
      "Inference:  82%|████████▏ | 97/119 [01:17<00:16,  1.33it/s]\n",
      "Inference:  82%|████████▏ | 98/119 [01:18<00:15,  1.33it/s]\n",
      "Inference:  83%|████████▎ | 99/119 [01:19<00:15,  1.33it/s]\n",
      "Inference:  84%|████████▍ | 100/119 [01:20<00:14,  1.33it/s]\n",
      "Inference:  85%|████████▍ | 101/119 [01:20<00:13,  1.33it/s]\n",
      "Inference:  86%|████████▌ | 102/119 [01:21<00:12,  1.33it/s]\n",
      "Inference:  87%|████████▋ | 103/119 [01:22<00:11,  1.33it/s]\n",
      "Inference:  87%|████████▋ | 104/119 [01:23<00:11,  1.33it/s]\n",
      "Inference:  88%|████████▊ | 105/119 [01:23<00:10,  1.33it/s]\n",
      "Inference:  89%|████████▉ | 106/119 [01:24<00:09,  1.33it/s]\n",
      "Inference:  90%|████████▉ | 107/119 [01:25<00:08,  1.33it/s]\n",
      "Inference:  91%|█████████ | 108/119 [01:26<00:08,  1.33it/s]\n",
      "Inference:  92%|█████████▏| 109/119 [01:26<00:07,  1.33it/s]\n",
      "Inference:  92%|█████████▏| 110/119 [01:27<00:06,  1.34it/s]\n",
      "Inference:  93%|█████████▎| 111/119 [01:28<00:05,  1.34it/s]\n",
      "Inference:  94%|█████████▍| 112/119 [01:29<00:05,  1.34it/s]\n",
      "Inference:  95%|█████████▍| 113/119 [01:29<00:04,  1.34it/s]\n",
      "Inference:  96%|█████████▌| 114/119 [01:30<00:03,  1.34it/s]\n",
      "Inference:  97%|█████████▋| 115/119 [01:31<00:02,  1.34it/s]\n",
      "Inference:  97%|█████████▋| 116/119 [01:32<00:02,  1.34it/s]\n",
      "Inference:  98%|█████████▊| 117/119 [01:32<00:01,  1.34it/s]\n",
      "Inference:  99%|█████████▉| 118/119 [01:33<00:00,  1.34it/s]\n",
      "Inference: 100%|██████████| 119/119 [01:35<00:00,  1.01s/it]\n",
      "Inference: 100%|██████████| 119/119 [01:35<00:00,  1.25it/s]\n",
      "Results exported to: outputs/ClassifierSegExperimentCV_224_epo30_bs32_lr0.0001_s42/2023-12-20_1507_VGG16_BN_Attention/ClassifierSegExperimentCV_224_epo30_bs32_lr0.0001_s42_2023-12-20_1507_VGG16_BN_Attention.csv\n",
      "Majority vote AUC: 0.92967\n",
      "Majority vote accuracy: 0.9299262381454162\n",
      "Majority vote kappa: 0.8597432658471125\n",
      "Target 0 - Precision: 0.91982, Recall: 0.94459, F-score: 0.93204, Sensitivity: 0.9445882962195753, Specificity: 0.914745308310992, Support: 1931\n",
      "Target 1 - Precision: 0.94098, Recall: 0.91475, F-score: 0.92768, Sensitivity: 0.914745308310992, Specificity: 0.9445882962195753, Support: 1865\n",
      "Figure(1500x600)\n",
      "(   {   'accuracy': 0.9299262381454162,\n",
      "        'auc': 0.9296668022652836,\n",
      "        'fscore': [0.9320388349514563, 0.9276780859162588],\n",
      "        'kappa': 0.8597432658471125,\n",
      "        'precision': [0.9198184568835098, 0.9409817981246553],\n",
      "        'recall': [0.9445882962195753, 0.914745308310992],\n",
      "        'sensitivity': [0.9445882962195753, 0.914745308310992],\n",
      "        'specificity': [0.914745308310992, 0.9445882962195753],\n",
      "        'support': [1931, 1865]},)\n"
     ]
    }
   ],
   "source": [
    "command = 'python ../inference.py --test_path \"../datasets/challenge1/val\" \\\n",
    "                    --experiment_name \"ClassifierSegExperimentCV\" \\\n",
    "                    --network_name \"VGG16_BN_Attention\" \\\n",
    "                    --max_epochs \"30\" \\\n",
    "                    --batch_size \"32\" \\\n",
    "                    --timeframe \"2023-12-20_1507\" \\\n",
    "                    --verbose \"2\" \\\n",
    "                    --ensemble \\\n",
    "                    --report'\n",
    "\n",
    "# run the function to excute the command\n",
    "_ = execute_cmd_realtime(command, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mask improved the ensemble in some metrics a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `VGG16_BN_Attention_P4` with one attention block from pool layer 4.Added `A.CenterCrop` to all transforms (including the val/test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
